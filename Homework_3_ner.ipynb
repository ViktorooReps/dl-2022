{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss9WR_zbQELO"
   },
   "source": [
    "# Практическое задание 3\n",
    "\n",
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB3vM2GfQELQ"
   },
   "source": [
    "## Введение\n",
    "\n",
    "### Постановка задачи\n",
    "\n",
    "В этом задании вы будете решать задачу извлечения именованных сущностей (Named Entity Recognition) - одну из самых распространенных в NLP наряду с задачей текстовой классификации.\n",
    "\n",
    "Данная задача заключается в том, что нужно классифицировать каждое слово / токен на предмет того, является ли оно частью именованной сущности (сущность может состоять из нескольких слов / токенов) или нет.\n",
    "\n",
    "Например, мы хотим извлечь имена и названия организаций. Тогда для текста\n",
    "\n",
    "    Yan    Goodfellow  works  for  Google  Brain\n",
    "\n",
    "модель должна извлечь следующую последовательность:\n",
    "\n",
    "    B-PER  I-PER       O      O    B-ORG   I-ORG\n",
    "\n",
    "где префиксы *B-* и *I-* означают начало и конец именованной сущности, *O* означает слово без тега. Такая префиксная система (*BIO*-разметка) введена, чтобы различать последовательные именованные сущности одного типа.\n",
    "Существуют и другие типы разметок, например *BILUO*, но в рамках данного практического задания сфокусируемся имеено на *BIO*.\n",
    "\n",
    "Решать NER задачу мы будем на датасете CoNLL-2003 с использованием рекуррентных сетей и моделей на базе архитектуры Transformer.\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Основные библиотеки:\n",
    " - [PyTorch](https://pytorch.org/)\n",
    " - [Transformers](https://github.com/huggingface/transformers)\n",
    " \n",
    "### Данные\n",
    "\n",
    "Данные лежат в архиве, который состоит из:\n",
    "\n",
    "- *train.tsv* - обучающая выборка. В каждой строке записаны: <слово / токен>, <тэг слова / токена>\n",
    "\n",
    "- *valid.tsv* - валидационная выборка, которую можно использовать для подбора гиперпарамеров и замеров качества. Имеет идентичную с train.tsv структуру.\n",
    "\n",
    "- *test.tsv* - тестовая выборка, по которой оценивается итоговое качество. Имеет идентичную с train.tsv структуру.\n",
    "\n",
    "Скачать данные можно здесь: [ссылка](https://github.com/dayyass/msu_task_3_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S5BCB1EfQan1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./venv/lib/python3.8/site-packages (1.21.6)\r\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.8/site-packages (1.0.2)\r\n",
      "Requirement already satisfied: tensorboard in ./venv/lib/python3.8/site-packages (2.9.0)\r\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.8/site-packages (1.13.0)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.8/site-packages (4.64.0)\r\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.8/site-packages (4.21.1)\r\n",
      "Requirement already satisfied: pytorch-crf in ./venv/lib/python3.8/site-packages (0.7.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in ./venv/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in ./venv/lib/python3.8/site-packages (from scikit-learn) (1.9.3)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in ./venv/lib/python3.8/site-packages (from tensorboard) (1.3.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./venv/lib/python3.8/site-packages (from tensorboard) (0.6.1)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in ./venv/lib/python3.8/site-packages (from tensorboard) (1.50.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./venv/lib/python3.8/site-packages (from tensorboard) (2.14.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venv/lib/python3.8/site-packages (from tensorboard) (0.4.6)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.8/site-packages (from tensorboard) (2.2.2)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./venv/lib/python3.8/site-packages (from tensorboard) (3.19.6)\r\n",
      "Requirement already satisfied: wheel>=0.26 in ./venv/lib/python3.8/site-packages (from tensorboard) (0.34.2)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venv/lib/python3.8/site-packages (from tensorboard) (1.8.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.8/site-packages (from tensorboard) (2.28.1)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.8/site-packages (from tensorboard) (44.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.8/site-packages (from tensorboard) (3.4.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./venv/lib/python3.8/site-packages (from torch) (11.7.99)\r\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.8/site-packages (from torch) (4.4.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./venv/lib/python3.8/site-packages (from torch) (11.7.99)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./venv/lib/python3.8/site-packages (from torch) (11.10.3.66)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./venv/lib/python3.8/site-packages (from torch) (8.5.0.96)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in ./venv/lib/python3.8/site-packages (from transformers) (0.10.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.8/site-packages (from transformers) (2022.10.31)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in ./venv/lib/python3.8/site-packages (from transformers) (0.12.1)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.8/site-packages (from transformers) (3.8.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.8/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: six>=1.5.2 in ./venv/lib/python3.8/site-packages (from grpcio>=1.24.3->tensorboard) (1.16.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in ./venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./venv/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.9.24)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in ./venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard) (5.0.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./venv/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./venv/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard) (3.10.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scikit-learn tensorboard torch tqdm transformers pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Thidpb9qQELS"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB\n",
      "NVIDIA A100-PCIE-40GB\n",
      "NVIDIA A100-PCIE-40GB\n",
      "NVIDIA A100-PCIE-40GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d_id in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(d_id))\n",
    "DEVICE = torch.device('cuda:0')\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-21 19:00:10--  https://data.deepai.org/conll2003.zip\r\n",
      "Resolving data.deepai.org (data.deepai.org)... 5.9.140.253\r\n",
      "Connecting to data.deepai.org (data.deepai.org)|5.9.140.253|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 982975 (960K) [application/x-zip-compressed]\r\n",
      "Saving to: ‘conll03/conll2003.zip’\r\n",
      "\r\n",
      "conll2003.zip       100%[===================>] 959.94K  4.23MB/s    in 0.2s    \r\n",
      "\r\n",
      "2022-11-21 19:00:11 (4.23 MB/s) - ‘conll03/conll2003.zip’ saved [982975/982975]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# download data in original segmentation\n",
    "!rm -rd conll03\n",
    "!mkdir conll03\n",
    "!wget https://data.deepai.org/conll2003.zip -P conll03"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  conll03/conll2003.zip\r\n",
      "  inflating: conll03/data/metadata   \r\n",
      "  inflating: conll03/data/test.txt   \r\n",
      "  inflating: conll03/data/train.txt  \r\n",
      "  inflating: conll03/data/valid.txt  \r\n",
      "metadata  test.txt  train.txt  valid.txt\r\n"
     ]
    }
   ],
   "source": [
    "!unzip -o conll03/conll2003.zip -d conll03/data\n",
    "!ls conll03/data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-21 19:00:12--  http://nlp.stanford.edu/data/glove.6B.zip\r\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\r\n",
      "--2022-11-21 19:00:13--  https://nlp.stanford.edu/data/glove.6B.zip\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\r\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\r\n",
      "--2022-11-21 19:00:13--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\r\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\r\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 862182613 (822M) [application/zip]\r\n",
      "Saving to: ‘glove/glove.6B.zip’\r\n",
      "\r\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.06MB/s    in 2m 39s  \r\n",
      "\r\n",
      "2022-11-21 19:02:54 (5.16 MB/s) - ‘glove/glove.6B.zip’ saved [862182613/862182613]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# download pretrained word embeddings\n",
    "!rm -rd glove\n",
    "!mkdir glove\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip -P glove"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove/glove.6B.zip\r\n",
      "  inflating: glove/data/glove.6B.50d.txt  \r\n",
      "  inflating: glove/data/glove.6B.100d.txt  \r\n",
      "  inflating: glove/data/glove.6B.200d.txt  \r\n",
      "  inflating: glove/data/glove.6B.300d.txt  \r\n",
      "glove.6B.100d.txt  glove.6B.200d.txt  glove.6B.300d.txt  glove.6B.50d.txt\r\n"
     ]
    }
   ],
   "source": [
    "!unzip -o glove/glove*.zip -d glove/data\n",
    "!ls glove/data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiDlmbY2QELT"
   },
   "source": [
    "Зафиксируем seed для воспроизводимости результатов (желательно делать **всегда**!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yt3ISg3aQELU"
   },
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set global seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k3Nhd3IQELY"
   },
   "source": [
    "## Часть 1. Подготовка данных (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qYjOMuPQELY"
   },
   "source": [
    "Первым делом нам нужно считать данные. Давайте напишем функцию, которая на вход принимает путь до одного из conll-2003 файла и возвращает два списка:\n",
    "- список списков слов / токенов (и соответствующий ему)\n",
    "- список списков тегов\n",
    "\n",
    "P.S. Сделаем данную функцию более гибкой, подавая на вход еще булеву переменную, считываем ли мы данные в *lowercase* или нет.\n",
    "\n",
    "**Задание. Реализуйте функцию read_conll2003.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wQdCfX2OQELZ"
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from typing import Iterable, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    sentences: List[List[str]]\n",
    "    labels: List[List[str]]\n",
    "\n",
    "\n",
    "def read_conll2003(path: Path, lower: bool = True, convert_to_iob: bool = False) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Prepares data in CoNNL like format.\n",
    "    \"\"\"\n",
    "\n",
    "    def entity_type(entity_label: str) -> str:\n",
    "        return entity_label.split('-')[-1]\n",
    "\n",
    "    prev_label = 'O'\n",
    "    def convert(lbl: str) -> str:\n",
    "        \"\"\"In IOB scheme we mark the beginning of a new entity if and only if it follows the entity of the same type\"\"\"\n",
    "        if lbl.startswith('B'):\n",
    "            curr_type = entity_type(lbl)\n",
    "            if prev_label == 'O' or entity_type(prev_label) != curr_type:\n",
    "                lbl = 'I-' + curr_type\n",
    "        return lbl\n",
    "\n",
    "\n",
    "    preprocess_token = str.lower if lower else lambda x: x\n",
    "    preprocess_label = convert if convert_to_iob else lambda x: x\n",
    "\n",
    "    def read_lines(f: StringIO) -> Iterable[Tuple[Tuple[str, str], bool, bool]]:\n",
    "        \"\"\"(word, label), new_sentence, new_doc\"\"\"\n",
    "        nonlocal prev_label\n",
    "\n",
    "        new_sentence = True\n",
    "        new_doc = True\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if not len(line):\n",
    "                new_sentence = True\n",
    "                prev_label = 'O'\n",
    "                continue\n",
    "\n",
    "            if line == '-DOCSTART- -X- -X- O':\n",
    "                new_doc = True\n",
    "                new_sentence = True\n",
    "                prev_label = 'O'\n",
    "                continue\n",
    "\n",
    "            token_info = line.split()\n",
    "            info_token, info_label = token_info[0], token_info[-1]\n",
    "            yield (preprocess_token(info_token), preprocess_label(info_label)), new_sentence, new_doc\n",
    "\n",
    "            new_sentence = False\n",
    "            new_doc = False\n",
    "            prev_label = info_label\n",
    "\n",
    "    curr_doc: Optional[Document] = None\n",
    "    curr_sentence: Optional[List[str]] = None\n",
    "    curr_labels: Optional[List[str]] = None\n",
    "\n",
    "    docs: List[Document] = []\n",
    "\n",
    "    with open(path) as dataset_file:\n",
    "        for (token, label), create_new_sentence, create_new_doc in read_lines(dataset_file):\n",
    "            if create_new_sentence or create_new_doc:\n",
    "                if curr_sentence is not None and len(curr_sentence):\n",
    "                    curr_doc.sentences.append(curr_sentence)\n",
    "                    curr_doc.labels.append(curr_labels)\n",
    "                curr_sentence = []\n",
    "                curr_labels = []\n",
    "\n",
    "            if create_new_doc:\n",
    "                if curr_doc is not None and len(curr_doc.sentences):\n",
    "                    docs.append(curr_doc)\n",
    "                curr_doc = Document([], [])\n",
    "\n",
    "            curr_sentence.append(token)\n",
    "            curr_labels.append(label)\n",
    "\n",
    "        if curr_doc is None:\n",
    "            raise ValueError(f'{path} dataset is empty!')\n",
    "\n",
    "        if len(curr_sentence):\n",
    "            curr_doc.sentences.append(curr_sentence)\n",
    "            curr_doc.labels.append(curr_labels)\n",
    "\n",
    "        if len(curr_doc.sentences):\n",
    "            docs.append(curr_doc)\n",
    "\n",
    "    print(f'Read {len(docs)} documents and {sum(map(lambda d: len(d.sentences), docs))} sentences from {path}')\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYm8xEvFQELb"
   },
   "source": [
    "Считаем все три файла:\n",
    "- *train.txt*\n",
    "- *valid.txt*\n",
    "- *test.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-inr1BPgQELb",
    "outputId": "125203fe-c32e-459d-e659-d1a999712147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 946 documents and 14041 sentences from conll03/data/train.txt\n",
      "Read 216 documents and 3250 sentences from conll03/data/valid.txt\n",
      "Read 231 documents and 3453 sentences from conll03/data/test.txt\n"
     ]
    }
   ],
   "source": [
    "train_docs = read_conll2003(Path('conll03/data/train.txt'))\n",
    "val_docs = read_conll2003(Path('conll03/data/valid.txt'))\n",
    "test_docs = read_conll2003(Path('conll03/data/test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOoNc1VUQELc"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HK8AcwWGQELd",
    "outputId": "d949fff7-3fb9-4e3e-c2fa-853d449f8314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu\tB-ORG\n",
      "rejects\tO\n",
      "german\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "british\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(train_docs[0].sentences[0], train_docs[0].labels[0]):\n",
    "    print(f\"{token}\\t{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8SqDeMJjF3Y",
    "outputId": "4939b43d-4fe6-4125-a383-6cb89b573d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cricket\tO\n",
      "-\tO\n",
      "leicestershire\tB-ORG\n",
      "take\tO\n",
      "over\tO\n",
      "at\tO\n",
      "top\tO\n",
      "after\tO\n",
      "innings\tO\n",
      "victory\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(val_docs[0].sentences[0], val_docs[0].labels[0]):\n",
    "    print(f\"{token}\\t{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddFE7p5kjF_p",
    "outputId": "b000a8f1-49b9-4022-905e-78077ee2e666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer\tO\n",
      "-\tO\n",
      "japan\tB-LOC\n",
      "get\tO\n",
      "lucky\tO\n",
      "win\tO\n",
      ",\tO\n",
      "china\tB-PER\n",
      "in\tO\n",
      "surprise\tO\n",
      "defeat\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(test_docs[0].sentences[0], test_docs[0].labels[0]):\n",
    "    print(f\"{token}\\t{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZ4Go3IXfDit",
    "outputId": "f5d3dc81-6aa2-4311-8168-b0c9dc0976d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert all(len(train_doc.sentences) == len(train_doc.labels) for train_doc in train_docs), \"Длины тренировочных token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
    "assert all(len(val_doc.sentences) == len(val_doc.labels) for val_doc in val_docs), \"Длины валидационных token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
    "assert all(len(test_doc.sentences) == len(test_doc.labels) for test_doc in test_docs), \"Длины тестовых token_seq и label_seq не совпадают, ошибка в функции read_conll2003\"\n",
    "\n",
    "assert train_docs[0].sentences[0] == ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'], \"Ошибка в тренировочном token_seq\"\n",
    "assert train_docs[0].labels[0] == ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'], \"Ошибка в тренировочном label_seq\"\n",
    "\n",
    "assert val_docs[0].sentences[0] == ['cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top', 'after', 'innings', 'victory', '.'], \"Ошибка в валидационном token_seq\"\n",
    "assert val_docs[0].labels[0] == ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], \"Ошибка в валидационном label_seq\"\n",
    "\n",
    "assert test_docs[0].sentences[0] == ['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.'], \"Ошибка в тестовом token_seq\"\n",
    "assert test_docs[0].labels[0] == ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O'], \"Ошибка в тестовом label_seq\"\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j96zKo6PQELd"
   },
   "source": [
    "Датасет CoNLL-2003 представлен в виде разметки **BIO**, где лейбл:\n",
    "- *B-{label}* - начало сущности *{label}*\n",
    "- *I-{label}* - продолжение сущности *{label}*\n",
    "- *O* - отсутсвие сущности\n",
    "\n",
    "Также существует другие разметки последовательностей, например **BILUO**. Подробнее с разметками можно ознакомится во вспомогательном ноутбуке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# read glove embeddings\n",
    "def read_glove(path: Path) -> Dict[str, np.ndarray]:\n",
    "    print(f'Extracting word embeddings from {path}')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print(f'Extracted {len(embeddings_index)} embeddings')\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def glove2vocab(path: Path) -> Dict[str, int]:\n",
    "    print(f'Building pretrained vocabulary from {path}')\n",
    "\n",
    "    vocab = {}\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for word_idx, line in enumerate(f):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vocab[word] = word_idx\n",
    "\n",
    "    print(f'Vocabulary size: {len(vocab)}')\n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting word embeddings from glove/data/glove.6B.50d.txt\n",
      "Extracted 400000 embeddings\n",
      "{'1.3775': array([-0.24171  , -0.23367  ,  0.10672  , -1.6023   ,  0.1244   ,\n",
      "       -0.016423 ,  0.1302   ,  0.70318  , -0.14301  ,  0.47307  ,\n",
      "       -0.67426  , -0.73478  ,  0.92795  , -0.2342   ,  0.1281   ,\n",
      "       -0.3318   ,  0.084984 , -1.0039   , -0.81191  ,  0.67333  ,\n",
      "        1.0034   ,  0.067612 , -0.67016  , -0.60906  , -0.018144 ,\n",
      "        0.34908  ,  0.41337  ,  0.043822 ,  0.22416  , -0.89374  ,\n",
      "       -1.5504   ,  0.7269   ,  0.41673  ,  0.34538  , -0.22609  ,\n",
      "        0.41702  ,  0.80483  , -0.097597 , -0.67301  , -0.30269  ,\n",
      "        0.77956  , -0.072954 ,  0.16443  , -0.0061933,  0.061141 ,\n",
      "       -0.28784  ,  0.58601  ,  0.47279  , -0.61084  , -0.72091  ],\n",
      "      dtype=float32),\n",
      " 'aqm': array([-1.1167e+00,  1.4057e-01,  3.6302e-01, -1.3836e-01, -1.4797e+00,\n",
      "       -9.8573e-01,  4.0487e-01, -3.9773e-01, -4.0102e-01,  3.4691e-01,\n",
      "        3.8857e-01,  2.9772e-01,  8.2807e-01, -2.4541e-01, -2.0565e-01,\n",
      "       -1.4533e-03,  3.8004e-01,  9.3941e-01,  1.0323e+00, -5.7793e-01,\n",
      "       -6.7443e-02,  1.2826e-01, -4.9012e-02, -5.2764e-01,  5.0883e-01,\n",
      "        1.3295e+00,  1.2772e+00, -2.8354e-01, -1.3242e+00,  9.9210e-01,\n",
      "       -1.1391e+00, -2.1213e-01, -4.0201e-01, -1.7518e-01, -5.4253e-01,\n",
      "        6.6579e-01, -5.4747e-01,  7.7698e-01,  4.4926e-01,  1.3868e-01,\n",
      "        3.0189e-01,  4.8314e-02, -7.9109e-02,  5.8375e-01,  1.4685e-01,\n",
      "       -4.6245e-01,  4.4275e-01,  2.1229e-01,  1.6195e-01, -7.8756e-01],\n",
      "      dtype=float32),\n",
      " 'chanty': array([ 0.23204 ,  0.025672, -0.70699 , -0.045465,  0.13989 , -0.62807 ,\n",
      "        0.72625 ,  0.34108 ,  0.44614 ,  0.16329 ,  0.15937 ,  0.74378 ,\n",
      "        1.1124  ,  1.0584  , -0.68263 , -0.51339 ,  0.45045 ,  0.10206 ,\n",
      "       -0.26206 ,  0.13301 , -0.75847 , -0.23059 ,  0.52357 , -0.29149 ,\n",
      "        0.36749 ,  0.96233 ,  0.25214 ,  0.57534 ,  0.40843 ,  0.46418 ,\n",
      "       -0.98081 , -0.407   ,  0.37957 ,  0.74278 , -0.53581 ,  0.042461,\n",
      "       -0.58834 , -0.63505 , -0.96662 ,  0.22057 , -0.095526, -0.29605 ,\n",
      "        0.38567 ,  0.13684 ,  0.59331 , -0.69486 ,  0.1241  , -0.18069 ,\n",
      "       -0.2583  , -0.039673], dtype=float32),\n",
      " 'corythosaurus': array([-0.042672 , -0.088106 , -0.31724  , -0.25209  , -0.26851  ,\n",
      "       -0.06615  ,  0.90325  , -0.13818  ,  0.3186   ,  0.30621  ,\n",
      "       -0.020125 ,  1.0509   ,  0.40654  , -0.10525  , -0.12052  ,\n",
      "       -0.54416  , -0.03742  ,  0.5367   ,  0.87692  ,  0.040133 ,\n",
      "       -0.20563  , -0.45572  ,  0.39592  , -0.0070575,  0.49928  ,\n",
      "        1.0726   , -0.71301  ,  0.50881  ,  0.50365  , -0.05629  ,\n",
      "       -1.1315   , -0.2297   ,  0.4061   ,  0.52096  , -0.481    ,\n",
      "        0.0092775, -0.096609 , -0.90419  , -0.58088  ,  0.30755  ,\n",
      "       -0.38187  , -0.91153  ,  0.11853  , -0.13034  ,  0.48774  ,\n",
      "       -0.99745  , -0.069557 ,  0.24963  ,  0.75791  ,  0.50679  ],\n",
      "      dtype=float32),\n",
      " 'katuna': array([-0.30016 , -0.80268 , -0.46637 , -0.29822 , -1.032   , -1.0705  ,\n",
      "        0.84562 ,  0.70225 ,  0.11996 , -0.7183  , -0.61271 , -0.92747 ,\n",
      "        0.29668 , -0.2894  , -0.21143 ,  0.27755 ,  0.19278 ,  0.26899 ,\n",
      "        0.67493 ,  0.99469 ,  0.18606 ,  0.15348 , -0.30083 ,  0.99714 ,\n",
      "        0.53874 ,  0.15414 ,  0.79983 ,  0.60903 ,  0.55637 ,  0.046631,\n",
      "       -0.85653 , -0.26881 , -0.094394,  0.59491 , -0.7241  ,  0.71182 ,\n",
      "       -0.31965 , -0.044493, -0.71945 ,  0.20368 ,  0.35178 ,  0.14789 ,\n",
      "       -0.015559,  0.16185 ,  0.5095  , -0.60983 ,  1.2486  ,  0.33713 ,\n",
      "       -0.22136 , -0.39756 ], dtype=float32),\n",
      " 'kronik': array([-0.60921  , -0.67218  ,  0.23521  , -0.11195  , -0.46094  ,\n",
      "       -0.0074616,  0.25578  ,  0.85632  ,  0.055977 , -0.23792  ,\n",
      "        0.067622 ,  0.48053  , -0.23417  , -0.11349  , -0.38991  ,\n",
      "        0.025498 ,  0.55526  , -0.32664  ,  0.088174 , -0.10024  ,\n",
      "       -0.40195  , -0.05636  ,  0.40048  ,  0.2107   , -0.032654 ,\n",
      "        0.93335  ,  0.9761   , -0.077802 ,  0.19778  ,  0.13785  ,\n",
      "       -1.19     ,  0.17476  ,  0.49433  , -0.047953 , -0.44248  ,\n",
      "        0.3163   ,  0.041216 , -0.21698  , -0.48888  , -0.48584  ,\n",
      "        0.67205  , -0.59822  , -0.20259  ,  0.39243  ,  0.028873 ,\n",
      "        0.030003 , -0.10617  , -0.11411  , -0.24901  , -0.12026  ],\n",
      "      dtype=float32),\n",
      " 'rolonda': array([-0.51181 ,  0.058706,  1.0913  , -0.55163 , -0.10249 , -0.1265  ,\n",
      "        0.99503 ,  0.079711, -0.16246 ,  0.56488 , -0.63306 , -0.48592 ,\n",
      "        0.76247 , -0.042514,  0.73388 ,  0.12552 ,  0.67302 ,  1.283   ,\n",
      "        0.14078 , -0.67687 ,  0.35145 , -0.078989, -0.068807, -0.15474 ,\n",
      "        0.51258 ,  1.1     ,  0.53208 ,  0.20631 ,  0.34584 ,  0.17114 ,\n",
      "       -1.5224  ,  0.27799 ,  0.026445,  0.14108 , -1.0146  , -0.045965,\n",
      "        0.79443 , -0.85389 , -0.18204 ,  0.041465,  0.024747,  0.20092 ,\n",
      "       -1.0851  , -0.13626 ,  0.35052 , -0.85891 ,  0.067858, -0.25003 ,\n",
      "       -1.125   ,  1.5863  ], dtype=float32),\n",
      " 'sandberger': array([ 0.072617, -0.51393 ,  0.4728  , -0.52202 , -0.35534 ,  0.34629 ,\n",
      "        0.23211 ,  0.23096 ,  0.26694 ,  0.41028 ,  0.28031 ,  0.14107 ,\n",
      "       -0.30212 , -0.21095 , -0.10875 , -0.33659 , -0.46313 , -0.40999 ,\n",
      "        0.32764 ,  0.47401 , -0.43449 ,  0.19959 , -0.55808 , -0.34077 ,\n",
      "        0.078477,  0.62823 ,  0.17161 , -0.34454 , -0.2066  ,  0.1323  ,\n",
      "       -1.8076  , -0.38851 ,  0.37654 , -0.50422 , -0.012446,  0.046182,\n",
      "        0.70028 , -0.010573, -0.83629 , -0.24698 ,  0.6888  , -0.17986 ,\n",
      "       -0.066569, -0.48044 , -0.55946 , -0.27594 ,  0.056072, -0.18907 ,\n",
      "       -0.59021 ,  0.55559 ], dtype=float32),\n",
      " 'sigarms': array([-0.74397  ,  0.082164 , -0.0091471,  0.4129   , -0.42255  ,\n",
      "        0.10125  , -0.18602  ,  0.21051  , -0.59037  ,  0.66988  ,\n",
      "       -0.13711  ,  0.63894  ,  0.042985 , -0.63123  , -0.62241  ,\n",
      "        0.024485 , -0.87917  , -0.53674  ,  0.56378  ,  0.11545  ,\n",
      "        0.37125  , -0.014403 ,  0.23307  ,  0.15689  , -0.76922  ,\n",
      "        1.1551   , -0.32294  , -0.52755  ,  0.31421  , -0.12624  ,\n",
      "       -1.8159   , -0.024966 ,  0.06498  ,  0.37671  ,  0.35349  ,\n",
      "       -0.87959  , -0.41025  , -0.80498  , -0.10885  ,  0.12705  ,\n",
      "        0.53364  , -0.93231  , -0.39097  ,  0.60638  ,  0.42173  ,\n",
      "        0.1062   , -0.14878  , -0.11773  , -0.097637 ,  0.093382 ],\n",
      "      dtype=float32),\n",
      " 'zsombor': array([-0.75898 , -0.47426 ,  0.4737  ,  0.7725  , -0.78064 ,  0.23233 ,\n",
      "        0.046114,  0.84014 ,  0.24371 ,  0.022978,  0.53964 , -0.36101 ,\n",
      "        0.94198 , -0.08498 , -0.17095 ,  0.52358 ,  0.38773 , -0.90057 ,\n",
      "       -0.052854,  0.64844 , -0.28436 , -0.58268 ,  0.47154 , -0.15341 ,\n",
      "        0.56085 ,  1.7462  ,  1.5916  , -0.60234 ,  0.60964 ,  0.83124 ,\n",
      "       -1.1914  ,  0.093531,  0.33981 ,  0.23403 , -0.61819 , -0.68542 ,\n",
      "        0.25952 , -0.27869 ,  0.035413,  0.58834 ,  0.45439 , -0.84254 ,\n",
      "        0.1065  , -0.059397,  0.090449,  0.30581 , -0.61424 ,  0.78954 ,\n",
      "       -0.014116,  0.6448  ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dict(list(read_glove(Path('glove/data/glove.6B.50d.txt')).items())[399990:400000]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SVL4USbQELe"
   },
   "source": [
    "### Подготовка словарей\n",
    "\n",
    "Чтобы обучать нейронную сеть, мы будем использовать два отображения:\n",
    "- {**token**}→{**token_idx**}: соответствие между словом / токеном и строкой в *embedding* матрице (начинается с 0);\n",
    "- {**label**}→{**label_idx**}: соответствие между тегом и уникальным индексом (начинается с 0);\n",
    "\n",
    "Теперь нам необходимо реализовать две функции:\n",
    "- get_token2idx\n",
    "- get_label2idx\n",
    "\n",
    "которые будут возвращать соответствующие словари.\n",
    "\n",
    "P.S. token2idx словарь должен также содержать специальные токены:\n",
    "- `<PAD>` - спецтокен для паддинга, так как мы собираемся обучать модели батчами\n",
    "- `<UNK>` - спецтокен для обработки слов / токенов, которых нет в словаре (актуально для инференса)\n",
    "\n",
    "Давайте для удобства дадим им idx 0 и 1 соответственно.\n",
    "\n",
    "P.P.S. В get_token2idx можно также добавить параметр *min_count*, который будет включать только слова превышающие определенную частоту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOnc3UHpQELf"
   },
   "source": [
    "Сначала соберем:\n",
    "- token2cnt - словарь из уникального слова / токена в количество это слова / токена в тренировочной выборке (важно, что только в тренировочной!)\n",
    "- label_set - список из уникальных тегов\n",
    "\n",
    "P.S. Также можно использовать стемминг для того, чтобы преобразовывать разные словоформы одного слова в один токен, но мы опустим этот момент.\n",
    "\n",
    "**Задание. Реализуйте функции get_token2idx и get_label2idx.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IthnXKsoo7A3"
   },
   "outputs": [],
   "source": [
    "def get_token_counts(docs: Iterable[Document], lower: bool = True):\n",
    "    postprocess = (lambda tok: tok.lower()) if lower else (lambda x: x)\n",
    "    return Counter([postprocess(token) for doc in docs for sentence in doc.sentences for token in sentence])\n",
    "\n",
    "token2cnt = get_token_counts(train_docs, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_v8YUM7QELg",
    "outputId": "77f22cb2-a640-43e5-996b-fcaa2020ce99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 8390),\n ('.', 7374),\n (',', 7290),\n ('of', 3815),\n ('in', 3621),\n ('to', 3424),\n ('a', 3199),\n ('and', 2872),\n ('(', 2861),\n (')', 2861)]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSm7B546nmDh",
    "outputId": "d6863a06-52a2-42da-859f-41097e761673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов в тренировочном датасете: 21009\n",
      "Количество слов встречающихся только один раз в тренировочном датасете: 10060\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество уникальных слов в тренировочном датасете: {len(token2cnt)}\")\n",
    "print(f\"Количество слов встречающихся только один раз в тренировочном датасете: {len([token for token, cnt in token2cnt.items() if cnt == 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRtCHt1QruSU"
   },
   "source": [
    "Как мы видим, у нас есть много слов, которые встречаются только один раз в датасете. Очевидно, что выучиться по ним у нас не получиться, мы только переобучимся, поэтому давайте выкинем такие слова при формировании нашего словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aCaPftCyQELi"
   },
   "outputs": [],
   "source": [
    "# используйте параметр min_count для того, чтобы отсекать слова частотой cnt < min_count\n",
    "\n",
    "def get_token2idx(token_counts: Dict[str, int], min_count: int, base_dict: Optional[Dict[str, int]] = None) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get mapping from tokens to indices to use with Embedding layer.\n",
    "    \"\"\"\n",
    "    if base_dict is None:\n",
    "        result = {'<PAD>': 0, '<UNK>': 1}\n",
    "        curr_idx = 2\n",
    "    else:\n",
    "        result = deepcopy(base_dict)\n",
    "        curr_idx = len(result)\n",
    "        if '<PAD>' not in result:\n",
    "            result['<PAD>'] = curr_idx\n",
    "            curr_idx += 1\n",
    "        if '<UNK>' not in result:\n",
    "            result['<UNK>'] = curr_idx\n",
    "            curr_idx += 1\n",
    "\n",
    "    for tok in filter(lambda t: t not in result, token_counts.keys()):\n",
    "        tok_count = token_counts[tok]\n",
    "        if tok_count < min_count:\n",
    "            continue\n",
    "\n",
    "        result[tok] = curr_idx\n",
    "        curr_idx += 1\n",
    "\n",
    "    print(f'Final vocabulary size: {len(result)}')\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_char2idx(tokens: Iterable[str], num_embedding: bool = False) -> Dict[str, int]:\n",
    "    result = {'<PAD>': 0, '<UNK>': 1}\n",
    "    curr_idx = 3 if num_embedding else 2\n",
    "    for char in set(chain.from_iterable(tokens)):\n",
    "        if char.isdecimal() and num_embedding:\n",
    "            result[char] = 2\n",
    "        else:\n",
    "            result[char] = curr_idx\n",
    "            curr_idx += 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFK130y-sLH4",
    "outputId": "35f031b5-9388-48c4-8f8a-e77b5a3e580f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocabulary size: 10951\n"
     ]
    },
    {
     "data": {
      "text/plain": "10951"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2idx = get_token2idx(token2cnt, min_count=2)\n",
    "len(t2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{'<PAD>': 0,\n '<UNK>': 1,\n '0': 2,\n '!': 3,\n '1': 4,\n ']': 5,\n 'u': 6,\n '$': 7,\n 't': 8,\n '7': 9,\n 'm': 10,\n '=': 11,\n '3': 12,\n 'h': 13,\n '2': 14,\n '@': 15,\n '5': 16,\n '+': 17,\n '8': 18,\n 'j': 19,\n '9': 20,\n \"'\": 21,\n 'f': 22,\n '`': 23,\n ',': 24,\n 'a': 25,\n '4': 26,\n ';': 27,\n '*': 28,\n '-': 29,\n '[': 30,\n 'd': 31,\n 'g': 32,\n 'e': 33,\n ':': 34,\n '?': 35,\n 'k': 36,\n 'y': 37,\n '&': 38,\n '%': 39,\n 'l': 40,\n 'i': 41,\n ')': 42,\n 'c': 43,\n 'w': 44,\n 'n': 45,\n '\"': 46,\n '(': 47,\n 'v': 48,\n '.': 49,\n 'q': 50,\n 's': 51,\n 'o': 52,\n '6': 53,\n '/': 54,\n 'r': 55,\n 'x': 56,\n 'z': 57,\n 'p': 58,\n 'b': 59}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2idx = get_char2idx(token2cnt.keys())\n",
    "c2idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "{'<PAD>': 0,\n '<UNK>': 1,\n '0': 2,\n '!': 3,\n '1': 2,\n ']': 4,\n 'u': 5,\n '$': 6,\n 't': 7,\n '7': 2,\n 'm': 8,\n '=': 9,\n '3': 2,\n 'h': 10,\n '2': 2,\n '@': 11,\n '5': 2,\n '+': 12,\n '8': 2,\n 'j': 13,\n '9': 2,\n \"'\": 14,\n 'f': 15,\n '`': 16,\n ',': 17,\n 'a': 18,\n '4': 2,\n ';': 19,\n '*': 20,\n '-': 21,\n '[': 22,\n 'd': 23,\n 'g': 24,\n 'e': 25,\n ':': 26,\n '?': 27,\n 'k': 28,\n 'y': 29,\n '&': 30,\n '%': 31,\n 'l': 32,\n 'i': 33,\n ')': 34,\n 'c': 35,\n 'w': 36,\n 'n': 37,\n '\"': 38,\n '(': 39,\n 'v': 40,\n '.': 41,\n 'q': 42,\n 's': 43,\n 'o': 44,\n '6': 2,\n '/': 45,\n 'r': 46,\n 'x': 47,\n 'z': 48,\n 'p': 49,\n 'b': 50}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_char2idx(token2cnt.keys(), num_embedding=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(61, 6.882716930839164)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(map(len, token2cnt.keys())), sum(map(len, token2cnt.keys())) / len(token2cnt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "g69HFZC7QELh"
   },
   "outputs": [],
   "source": [
    "# Функция для сортировки тегов, чтобы сначала был тег O, потом теги B- и только после теги I- (можно задать вручную)\n",
    "\n",
    "def sort_labels_func(x: str) -> int:\n",
    "    if x == \"O\":\n",
    "        return 0\n",
    "    elif x.startswith(\"B-\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "label_set = sorted(\n",
    "    set(label for doc in train_docs for sentence_labels in doc.labels for label in sentence_labels),\n",
    "    key=lambda x: (sort_labels_func(x), x),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VI_3m4qbQELi",
    "outputId": "116bfcab-c56b-4ad9-b0f8-894ead418318"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['O', 'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "t6i51GPtQELj"
   },
   "outputs": [],
   "source": [
    "def get_label2idx(labels: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Get mapping from labels to indices.\n",
    "    \"\"\"\n",
    "\n",
    "    result: Dict[str, int] = {}\n",
    "\n",
    "    for label_idx, label in enumerate(labels):\n",
    "        result[label] = label_idx\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XW6fK0HtQELk",
    "outputId": "17d18b48-55fa-4603-b342-e61409199577"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'O': 0,\n 'B-LOC': 1,\n 'B-MISC': 2,\n 'B-ORG': 3,\n 'B-PER': 4,\n 'I-LOC': 5,\n 'I-MISC': 6,\n 'I-ORG': 7,\n 'I-PER': 8}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx = get_label2idx(label_set)\n",
    "label2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U13l-2IOQELk"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7U7bMrHQELl",
    "outputId": "53d80294-e80b-4536-acfa-99bf171d2e40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD>\t0\n",
      "<UNK>\t1\n",
      "eu\t2\n",
      "german\t3\n",
      "call\t4\n",
      "to\t5\n",
      "boycott\t6\n",
      "british\t7\n",
      "lamb\t8\n",
      ".\t9\n"
     ]
    }
   ],
   "source": [
    "for token, idx in list(t2idx.items())[:10]:\n",
    "    print(f\"{token}\\t{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hp75V-o2QELl",
    "outputId": "7f85dca1-eea6-4789-e261-1552bcaf91d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\t0\n",
      "B-LOC\t1\n",
      "B-MISC\t2\n",
      "B-ORG\t3\n",
      "B-PER\t4\n",
      "I-LOC\t5\n",
      "I-MISC\t6\n",
      "I-ORG\t7\n",
      "I-PER\t8\n"
     ]
    }
   ],
   "source": [
    "for label, idx in label2idx.items():\n",
    "    print(f\"{label}\\t{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYb4BdAUhNzk",
    "outputId": "dc62e8d8-7b62-4638-c436-aaf657dc5287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocabulary size: 21011\n",
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_token2idx(token2cnt, min_count=1)) == 21011, \"Ошибка в длине словаря, скорее всего неверно реализован min_count\"\n",
    "assert len(t2idx) == 10951, \"Неправильная длина token2idx, скорее всего неверно реализован min_count\"\n",
    "assert len(label2idx) == 9, \"Неправильная длина label2idx\"\n",
    "\n",
    "assert list(t2idx.items())[:10] == [('<PAD>', 0), ('<UNK>', 1), ('eu', 2), ('german', 3), ('call', 4), ('to', 5), ('boycott', 6), ('british', 7), ('lamb', 8), ('.', 9)], \"Неправильно сформированный token2idx\"\n",
    "assert label2idx == {'O': 0, 'B-LOC': 1, 'B-MISC': 2, 'B-ORG': 3, 'B-PER': 4, 'I-LOC': 5, 'I-MISC': 6, 'I-ORG': 7, 'I-PER': 8}, \"Неправильно сформированный label2idx\"\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItPs1DmOQELm"
   },
   "source": [
    "### Подготовка датасета и загрузчика\n",
    "\n",
    "Обычно нейронные сети обучаются батчами. Это означает, что каждое обновление весов нейронной сети происходит на основе нескольких последовательностей. Технической деталью является необходимость дополнить все последовательности внутри батча до одной длины.\n",
    "\n",
    "Из предыдущего практического задания вы должны знать о `Dataset`'е (`torch.utils.data.Dataset`) - структура данных, которая хранит и может по индексу отдавать данные для обучения. Датасет должен наследоваться от стандартного PyTorch класса Dataset и переопределять методы `__len__` и `__getitem__`.\n",
    "\n",
    "Метод `__getitem__` должен возвращать индексированную последовательность и её теги.\n",
    "\n",
    "**Не забудьте** про `<UNK>` спецтокен для неизвестных слов!\n",
    "    \n",
    "Давайте напишем кастомный датасет под нашу задачу, который на вход (метод `__init__`) будет принимать:\n",
    "- token_seq - список списков слов / токенов\n",
    "- label_seq - список списков тегов\n",
    "- token2idx\n",
    "- label2idx\n",
    "\n",
    "и возвращать из метода `__getitem__` два int64 тензора (`torch.LongTensor`) из индексов слов / токенов в сэмпле и индексов соответвующих тегов:\n",
    "\n",
    "**Задание. Реализуйте класс датасета NERDataset.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "kdZvnUUpQELm"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import starmap, chain\n",
    "from torch import LongTensor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenizedDocument:\n",
    "    char_ids: List[LongTensor]\n",
    "    token_ids: List[LongTensor]\n",
    "    label_ids: List[LongTensor]\n",
    "\n",
    "\n",
    "def group_sentences(doc: TokenizedDocument, max_sequence_length: int) -> Iterable[Tuple[LongTensor, LongTensor, LongTensor]]:\n",
    "    curr_length = 0\n",
    "    grouped_chars: List[LongTensor] = []\n",
    "    grouped_sentences: List[LongTensor] = []\n",
    "    grouped_label_ids: List[LongTensor] = []\n",
    "\n",
    "    for char_ids, sentence, label_ids in zip(doc.char_ids, doc.token_ids, doc.label_ids):\n",
    "        if len(sentence) > max_sequence_length:\n",
    "            print('Extra long sentence detected!!')\n",
    "\n",
    "        if len(sentence) + curr_length < max_sequence_length or not len(grouped_sentences):\n",
    "            grouped_chars.append(char_ids)\n",
    "            grouped_sentences.append(sentence)\n",
    "            grouped_label_ids.append(label_ids)\n",
    "            curr_length += len(sentence)\n",
    "            continue\n",
    "\n",
    "        yield torch.concat(grouped_chars), torch.concat(grouped_sentences), torch.concat(grouped_label_ids)\n",
    "        grouped_chars = [char_ids]\n",
    "        grouped_sentences = [sentence]\n",
    "        grouped_label_ids = [label_ids]\n",
    "        curr_length = len(sentence)\n",
    "\n",
    "    if len(grouped_sentences):\n",
    "        yield torch.concat(grouped_chars), torch.concat(grouped_sentences), torch.concat(grouped_label_ids)\n",
    "\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for NER.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            docs: Iterable[Document],\n",
    "            char2idx: Dict[str, int],\n",
    "            token2idx: Dict[str, int],\n",
    "            label2idx: Dict[str, int],\n",
    "            max_token_length: int,\n",
    "            max_sequence_length: int,\n",
    "            lower: bool\n",
    "    ):\n",
    "        self.char2idx = char2idx\n",
    "        self.token2idx = token2idx\n",
    "        self.label2idx = label2idx\n",
    "\n",
    "        # group sentences into examples of max_sequence_length length\n",
    "\n",
    "        def chars_processor(tokens: List[str]) -> LongTensor:\n",
    "            return torch.stack(list(map(\n",
    "                partial(self.process_chars, max_token_length=max_token_length, char2idx=self.char2idx),\n",
    "                tokens\n",
    "            ))).long()\n",
    "\n",
    "        tokens_processor = partial(self.process_tokens, token2idx=self.token2idx, lower=lower)\n",
    "        labels_processor = partial(self.process_labels, label2idx=self.label2idx)\n",
    "\n",
    "        tokenized_docs = starmap(\n",
    "            TokenizedDocument,\n",
    "            map(lambda doc: (\n",
    "                list(map(chars_processor, doc.sentences)),\n",
    "                list(map(tokens_processor, doc.sentences)),\n",
    "                list(map(labels_processor, doc.labels))\n",
    "            ), docs)\n",
    "        )\n",
    "\n",
    "        sentence_grouper = partial(group_sentences, max_sequence_length=max_sequence_length)\n",
    "\n",
    "        self._examples: List[Tuple[LongTensor, ...]] = list(chain.from_iterable(map(sentence_grouper, tokenized_docs)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._examples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[LongTensor, ...]:\n",
    "        return self._examples[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def process_chars(\n",
    "            token: str,\n",
    "            char2idx: Dict[str, int],\n",
    "            max_token_length: int,\n",
    "            pad: str = \"<PAD>\",\n",
    "            unk: str = \"<UNK>\"\n",
    "    ) -> LongTensor:\n",
    "        \"\"\"\n",
    "        Transform list of tokens into list of tokens' indices.\n",
    "        \"\"\"\n",
    "        unk_idx = char2idx[unk]\n",
    "        chars = list(token[:max_token_length]) + (max_token_length - len(token)) * [pad]\n",
    "        return torch.tensor(list(map(lambda tok: char2idx.get(tok, unk_idx), chars)), dtype=torch.long).long()\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_tokens(tokens: List[str], token2idx: Dict[str, int], unk: str = \"<UNK>\", lower: bool = False) -> LongTensor:\n",
    "        \"\"\"\n",
    "        Transform list of tokens into list of tokens' indices.\n",
    "        \"\"\"\n",
    "        preprocess = str.lower if lower else lambda x: x\n",
    "        unk_idx = token2idx[unk]\n",
    "        return torch.tensor(list(map(lambda tok: token2idx.get(preprocess(tok), unk_idx), tokens)), dtype=torch.long).long()\n",
    "\n",
    "    @staticmethod\n",
    "    def process_labels(labels: List[str], label2idx: Dict[str, int]) -> LongTensor:\n",
    "        \"\"\"\n",
    "        Transform list of labels into list of labels' indices.\n",
    "        \"\"\"\n",
    "        return  torch.tensor(list(map(label2idx.__getitem__, labels)), dtype=torch.long).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCvaPJERQELn"
   },
   "source": [
    "Создадим три датасета:\n",
    "- *train_dataset*\n",
    "- *valid_dataset*\n",
    "- *test_dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bUMsSNkoQELn"
   },
   "outputs": [],
   "source": [
    "train_ds = NERDataset(\n",
    "    train_docs,\n",
    "    char2idx=c2idx, token2idx=t2idx, label2idx=label2idx,\n",
    "    max_token_length=20, max_sequence_length=512,\n",
    "    lower=False\n",
    ")\n",
    "valid_ds = NERDataset(\n",
    "    val_docs,\n",
    "    char2idx=c2idx, token2idx=t2idx, label2idx=label2idx,\n",
    "    max_token_length=20, max_sequence_length=512,\n",
    "    lower=False\n",
    ")\n",
    "test_ds = NERDataset(\n",
    "    test_docs,\n",
    "    char2idx=c2idx, token2idx=t2idx, label2idx=label2idx,\n",
    "    max_token_length=20, max_sequence_length=512,\n",
    "    lower=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQIq1pAWQELo"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_Scync0QELo",
    "outputId": "1669dbf8-2b23-40e0-9012-430cd44c9bc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[33,  6,  0,  ...,  0,  0,  0],\n         [55, 33, 19,  ...,  0,  0,  0],\n         [32, 33, 55,  ...,  0,  0,  0],\n         ...,\n         [52, 48, 33,  ...,  0,  0,  0],\n         [41, 10, 58,  ...,  0,  0,  0],\n         [49,  0,  0,  ...,  0,  0,  0]]),\n tensor([  2,   1,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n          15,  16,  17,  18,  19,  20,  21,  22,   3,  23,   5,  24,   5,   1,\n           7,   8,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,   5,  35,\n           9,  36,  37,  38,   5,  14,  15,  39,  37,  40,  41,  42,  43,  17,\n          18,  44,  24,  45,  46,  47,  48,  49,  50,  51,  52,  25,  14,  53,\n          23,  54,   1,   9,  55,  56,  57,  58,  59,  60,  61,  62,  63,  56,\n          57,  58,  64,  60,  65,  66,  20,  67,  55,  14,  16,  37,  68,  69,\n           1,  70,  71,   1,  72,  73,  74,  75,   9,  76,  17,  77,  53,  78,\n          54,  79,  80,  81,  20,  54,  82,  83,  84,  54,  85,  20,  45,  33,\n          86,  87,  14,  15,  39,   9,  76,  17,  73,  88,  89,  90,  87,   2,\n          91,  92,  93,  94,   5,  95,  35,  96,  67,  97,  80,  98,  99,  48,\n          14, 100,  80, 101, 102, 103,  54,  73, 104, 105,  80,   1, 106,   5,\n         107, 100, 108,   9,  94, 109,   1, 110, 111, 112,  48,  52,  80, 113,\n          83, 114, 115, 116,  35, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n          29,  30,  31,   9, 126,  94, 127,   5, 128, 129,  88, 111,  14,   2,\n          37, 130,  40,  41,  67,   1, 101, 108, 131,  67, 132,  81,  61,  84,\n          54, 133, 134, 135,  54, 136,  73, 137, 138,   5, 100, 108,   9, 139,\n          91, 140,   1, 141,   1, 142, 143, 144,  94, 145, 146,   2,  91, 147,\n         148, 149, 150, 151,   1, 152, 153,  55, 154,   1,   9,  55,   9, 136,\n         113,  80,  52, 155,  94,  37,  88,   9,  14,   2,  37,  53,  40,  80,\n           1, 156, 157, 158,   5, 159,  14, 160, 161, 162,  90,  80, 163, 164,\n           5,  14, 165,  40, 131,   9,  35, 166, 167, 168, 169,   5, 118, 170,\n          67,  73, 171,  31, 172,   5, 123, 173, 174, 175,   5, 166, 168, 176,\n           5, 177, 153, 178, 179, 101, 180,   9,   7, 181, 182,  18,  19, 135,\n          54,  60, 183,   5, 100, 108,  48, 184,  35,  67, 126, 185, 186,  83,\n           3, 187,  23,   5,  24,   5, 188,   7,   8, 189, 190,  24, 191, 192,\n           9,  55, 193,  56, 166,   5,  33, 194,   1, 150, 174, 195,  50,  49,\n         157, 196,   5, 197,  36,  37, 198,  67,  55, 199, 200, 181, 148,  39,\n         122,   1, 124, 201, 202, 203, 204,  17,  18, 205, 206,   9, 207, 208,\n         209, 210,   5, 107, 211, 108, 111, 212, 213, 214, 215, 216, 111,  73,\n           7, 217, 218, 219, 117, 118, 146, 220, 172,   5,  29,  30,  31,  87,\n         221, 222, 223,   9,  36, 224, 225,  35,  48,  52,  89, 226,  67, 227,\n         228, 150, 229, 230,   9,  20, 231, 215, 232, 233, 150,   7, 234,  67,\n         235, 236, 237, 150, 238, 230,   9]),\n tensor([3, 0, 2, 0, 0, 0, 2, 0, 0, 4, 8, 1, 0, 0, 3, 7, 0, 0, 0, 0, 0, 0, 2, 0,\n         0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n         3, 7, 0, 0, 0, 4, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n         0, 0, 4, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 4, 8,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 6, 6,\n         0, 2, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n         4, 8, 8, 0, 0, 0, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 1, 0, 1, 0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n         0, 0, 0, 3, 7, 7, 7, 7, 0, 3, 0, 0, 4, 8, 8, 0, 0, 3, 7, 0, 1, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyAazaLzjQ-K",
    "outputId": "ed864972-9051-4b85-e206-30e59d9971c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[43, 55, 41,  ...,  0,  0,  0],\n         [29,  0,  0,  ...,  0,  0,  0],\n         [40, 33, 41,  ...,  0,  0,  0],\n         ...,\n         [22, 52, 55,  ...,  0,  0,  0],\n         [ 8, 13, 55,  ...,  0,  0,  0],\n         [49,  0,  0,  ...,  0,  0,  0]]),\n tensor([ 1736,   570,  1776,   197,   686,   145,   348,   111,  1818,  1557,\n             9,   247, 10678,   885,  4305,  2002,  1453,  1779,   678,  1891,\n            66,  6269,    18,   968,   134,  1776,  1518,  1752,    87,   146,\n          1818,    80,  3090,  2544,   215,   357,   840,     5,   197,   686,\n           145,    14,  1568,   150,    14,  1737,  1738,     9,   184,  3881,\n            18,   348,    67,  4631,    67,   941,    33,     1,   134,  1708,\n           354,  1767,    67,  6509,    80,  1800,   377,  1198,   215,    18,\n          1557,   434,  1769,  2844,   282,    66,  2428,   334,   215,   184,\n             1,  1528,   659,  1794,     9,   111,  1860,  1752,   694,    66,\n          4788,    18,    14,   344,  2856,   145,  9317,  2857,    67,  1776,\n          1060,   184,   578,  1818,    87,  1836,  2544,   903,  1355,  4728,\n           694,    66,     1,    22,  1186,     1,  2404,  9279,  1714,   876,\n            66,  4788,     9,  5905,    87,     1,    67,  1752,  2525,    73,\n          4379,  1339,     5,   184,   452,  1818,   903,  1779,  5117,   215,\n             5,     1,   960,   694,    66,     1,     9,  1767,    67,  1134,\n            67,  2175,  2883,     5,     1,   184,   348,   864,   111,     1,\n          1826,    80,    10,    61,   370,   960,    73,   993,  8950,    18,\n           184,  1528,   659,  1809,   145,  1808,     9,  1826,    67,   373,\n          1313,     5,  1186,    37,  6885,  4454,    67,  4920,     1,    67,\n           129,   578,  1738,  4715,   150,    14,  1594,    67,   134,  1767,\n          4828,     1,    80,   678,    73,   578,  1818,   198,   150,  2441,\n             9,    87,    14,  1361,  1809,   142,  3651,    83,   275,    73,\n             1,  4932,   126,  5303,    61,   142,     1,   184,  4241,    67,\n          1714,  1891,    66,   597,   215,  6304,  2838,    80,  1704,   960,\n          6403,    18,  2837,    66,  1621,    80,  6911,    66,  4717,     9,\n           145,    14,  1817,    67,  1800,  2162,  1577,  1839,    67,  1888,\n          1889,     1,    87,  1186,    67,  1095,     5,  3851,   129,  3731,\n           134,    76,  4451,   129,  1891,    66,  1192,    18,    19,    22,\n          4315,   696,   694,    18,   968,   215,    14,  1528,   659,  1802,\n             9,    76,    54,  1283,   155,    87,  1186, 10164,  1440,     1,\n           290,  2844,  1419,   134,  1800,  1198,    18,     1,    66,   966,\n            67,    73,   198,   150,     1,     9,  6509,   744,   282,    14,\n          4456,    66,   184,   578,  1738,  1708,   675,     1,    87,     1,\n          1807,     5,  5072,    66,  1621,   215,   184,   452,  1818,    67,\n           629,   493,  2544,  1587,    48,     1,   146,  1818,  3138,     9,\n           303,  1477,   520,   678,   879,    66,  2441,   126,  1577,  2354,\n            67,     1,    67,    80,  1442,     1,    67,  4798,    67,   678,\n          6509,     5,     1,    80,    73,   578,  1818,   198,   150,     1,\n             9,   111,    14,     1,   150,  4627,    14,   344,  1254,   150,\n           184,  1528,  8201,  4849,    87,    14,  4719,    67,  1769,  5117,\n           282,    73,  5057,     5,     1,  1794,    66,     1,     9,   294,\n           285,   389,   282,    87,    73,     1,  4490,    48,  1427,  2388,\n           126,     1,  1999,  2000,  1683,     1,   678,  1891,    66,  3224,\n             9,    87,     1,  1769,   142,  4828,  1780,    66,   876,     9]),\n tensor([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 6, 0, 4, 8, 0, 0, 0, 0, 0, 0,\n         0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0,\n         3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n         1, 5, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 8, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 0, 4, 8, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 3, 0, 1, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 4, 8, 0, 0, 0, 0, 0, 1,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 4, 8, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 8, 0, 0, 0, 0, 0, 4, 8,\n         0, 0, 0, 0, 4, 8, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 0, 2, 0, 0, 4, 8, 0, 0, 0, 0, 0, 0,\n         0, 3, 0, 0, 0, 0, 0, 0]))"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHuuh3YmjRNt",
    "outputId": "5f214ff3-7bbb-4b7c-8c9c-a861562b09cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[51, 52, 43,  ...,  0,  0,  0],\n         [29,  0,  0,  ...,  0,  0,  0],\n         [19, 25, 58,  ...,  0,  0,  0],\n         ...,\n         [52, 45, 33,  ...,  0,  0,  0],\n         [32, 25, 10,  ...,  0,  0,  0],\n         [49,  0,  0,  ...,  0,  0,  0]]),\n tensor([ 1515,   570,  1433,  1728,  4892,  2013,    67,   309,   215,  3156,\n          3138,     9,     1,     1,     1,    67,   720,   950,     1,     1,\n          1433,  1160,    14,  2639,   150,   184,  4391,  1526,  1708,    22,\n            73,  4892,  1519,  2013,   659,   588,   215,    73,   353,  1827,\n          1738,  1528,    18,   968,     9,   126,   309,  3683,   184,  3165,\n             1,   960,   215,    14,   452,  1528,   150,    14,   353,    67,\n          5514,     5,    73,  3156,  2028,  3138,     5,     1,     1,     9,\n           309,   983,   831,   150,    14,  1528,    80,  3683,  3072,  3241,\n          4843,    25,    14,  2074,  2010,   977,     1,  1885,     1,     1,\n           678,  4932,   150,    73,     1,   667,  6592,     5,     1,    14,\n          4854,   686,    14,  3221,   331,     1,    80,   275,   146,  5490,\n          1206,     9,     1,     1,  2844,  3763,   150,    14,  2013,   215,\n          3187,   334,    67,  4853,   146,     1,  1020,  4823,  2503,    48,\n          1619,   815,    14,  3106,     9,    14,   287,  4589,   994,    54,\n          3167,   215,   146,  4391,  1526,  2655,  2125,    66,    14,   578,\n           334,     9,   688,  2429,    14,  4391,  1590,  1708,   357,  1061,\n          2202,    67,     1,   157,   215,    14,  2655,   134,     1,     9,\n           357,  2046,    48,   667,  8502,   215,    14,    89,   879,  2633,\n          2553,  1433,     5,  3243,    48,  1336,    80,  6283,   377,   876,\n          1267,    48,   184,   344,   149,   659,   588,     9,     1,     1,\n          1880,    14,  1584,   215,    14,     1,  2010,    67,  3136,     5,\n          1568,    73,     1,     1,  4176,   386,    14,   627,   387,   173,\n          6761,     1,     1,   609,     5,   166,  9066,   126,  1731,  2553,\n             5,     1,   275,    14,  1206,     9,    20,    54,    14,   452,\n          5153,     1,    87,   588,   215,  1891,  2633,     9,  5269,   772,\n             1,   426,     5,     1,    73,   167,  4854,   275,    14,  3106,\n           215,    14,  1537,  2010,   126,   136,  7190,     5,     1,    20,\n           275,    14,   348,  7823,   150,     1,    37,   387,     9,     1,\n             1,   142,  1394,   588,    14,   198,    22,    73,     1,  6592,\n           215,    14,  2541,  2010,     9,  1433,  1731,     1,     1,     5,\n            14,   627,  2034,  3106,    66,   831,   150,    14,  1927,   126,\n         10877,     1,    14,   627,  2639,     9,     1,  4407,   584,  3891,\n             1,     1,   294,   990,     9,  1433,  2168,     1,     1,    17,\n           342,   148,   148,    14,   627,   850,   387,  9380,  4892,    66,\n           529,     9,    14,   631,  1880,   161,    80,  1731,  2126,     1,\n            80,  6047,   167,  2838,   173,  2844,    20,  4394,    66,   529,\n             9,   148,   148,  1433,    67,     1,   150,    14,  1410,  1526,\n           215,  1354,    80,  3157,  7835,   215,    14,  1410,    87,     1,\n            67,   157,  8331,     5,     1,   184,  1708,   636,     9,     1,\n           946,  1739,   825,    80,  1461,  2212,   197,    18,  2228,    18,\n          3326,   215,   353,    73,  1741,     9,   377,  1891,  2517,   157,\n           805,    22,   710,  3226,  2574,    48,   710,  1927,     9]),\n tensor([0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 8, 1, 0, 1, 5, 5, 0, 1, 0, 0, 0,\n         0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 4, 8, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 4, 8, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2,\n         6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 1, 0, 4, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 0, 0,\n         0, 2, 0, 0, 0, 4, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, 8, 0, 0, 1, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 2, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 8, 0, 0, 0, 0, 0, 2, 0,\n         0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 1, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gox6uyF2idwZ",
    "outputId": "8abb0480-9913-4f8a-d50e-1713611cbc9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я по-другому формирую датасеты, поэтому эти тесты не пройдутся\n"
     ]
    }
   ],
   "source": [
    "# assert len(train_dataset) == 14986, \"Неправильная длина train_dataset\"\n",
    "# assert len(valid_dataset) == 3465, \"Неправильная длина valid_dataset\"\n",
    "# assert len(test_dataset) == 3683, \"Неправильная длина test_dataset\"\n",
    "#\n",
    "# assert torch.equal(train_dataset[0][0], torch.tensor([2,1,3,4,5,6,7,8,9])), \"Неправильно сформированный train_dataset\"\n",
    "# assert torch.equal(train_dataset[0][1], torch.tensor([3,0,2,0,0,0,2,0,0])), \"Неправильно сформированный train_dataset\"\n",
    "#\n",
    "# assert torch.equal(valid_dataset[0][0], torch.tensor([1737,571,1777,197,687,145,349,111,1819,1558,9])), \"Неправильно сформированный valid_dataset\"\n",
    "# assert torch.equal(valid_dataset[0][1], torch.tensor([0,0,3,0,0,0,0,0,0,0,0])), \"Неправильно сформированный valid_dataset\"\n",
    "#\n",
    "# assert torch.equal(test_dataset[0][0], torch.tensor([1516,571,1434,1729,4893,2014,67,310,215,3157,3139,9])), \"Неправильно сформированный test_dataset\"\n",
    "# assert torch.equal(test_dataset[0][1], torch.tensor([0,0,1,0,0,0,0,4,0,0,0,0])), \"Неправильно сформированный test_dataset\"\n",
    "#\n",
    "# print(\"Тесты пройдены!\")\n",
    "\n",
    "\n",
    "\n",
    "print('Я по-другому формирую датасеты, поэтому эти тесты не пройдутся')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWjJuAk7QELp"
   },
   "source": [
    "Для того, чтобы дополнять последовательности паддингом, будем использовать параметр `collate_fn` класса `DataLoader`.\n",
    "\n",
    "Принимая последовательность пар тензоров для предложений и тегов, необходимо дополнить все последовательности до последовательности максимальной длины в батче.\n",
    "\n",
    "Используйте для дополнения спецтокен `<PAD>` для последовательностей слов / токенов и -1 для последовательностей тегов.\n",
    "\n",
    "**hint**: удобно использовать метод **torch.nn.utils.rnn**. Обратите особое внимание на параметр *batch_first*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZiJVM5qQELp"
   },
   "source": [
    "`Collator` можно реализовать двумя способами:\n",
    "- класс с методом `__call__`\n",
    "- функцию\n",
    "\n",
    "Мы пойдем первым путем.\n",
    "\n",
    "Инициализировать экземпляр класса `Collator` (метод `__init__`) с помощью двух параметров:\n",
    "- id `<PAD>` спецтокена для последовательностей слов / токенов\n",
    "- id `<PAD>` спецтокена для последовательностей тегов (значение -1)\n",
    "\n",
    "Метод `__call__` на вход принимает батч, а именно список кортежей того, что нам возвращается из датасета. В нашем случае это список кортежей двух int64 тензоров - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
    "\n",
    "На выходе мы хотим получить два тензора:\n",
    "- западденные индексы слов / токенов\n",
    "- западденные индексы тегов\n",
    "    \n",
    "P.S. `<PAD>` значение нужно для того, чтобы при подсчете лосса легко отличать западдированные токены от других. Можно использовать параметр *ignore_index* при инициализации лосса.\n",
    "\n",
    "**Задание. Реализуйте класс коллатора NERCollator.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "LNHNwoLnQELp"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class NERCollator:\n",
    "    \"\"\"\n",
    "    Collator that handles variable-size sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, char_padding_value: int, token_padding_value: int, label_padding_value: int):\n",
    "        self.char_padding_value = char_padding_value\n",
    "        self.token_padding_value = token_padding_value\n",
    "        self.label_padding_value = label_padding_value\n",
    "\n",
    "    def __call__(self, batch: List[Tuple[LongTensor, ...]]) -> Dict[str, LongTensor]:\n",
    "        chars, tokens, labels = zip(*batch)\n",
    "        return {\n",
    "            \"char_ids\": pad_sequence(chars, batch_first=True, padding_value=self.char_padding_value).long(),\n",
    "            \"token_ids\": pad_sequence(tokens, batch_first=True, padding_value=self.token_padding_value).long(),\n",
    "            \"labels\": pad_sequence(labels, batch_first=True, padding_value=self.label_padding_value).long()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "nZUMwVQTQELq"
   },
   "outputs": [],
   "source": [
    "coll = NERCollator(\n",
    "    char_padding_value=c2idx['<PAD>'],\n",
    "    token_padding_value=t2idx[\"<PAD>\"],\n",
    "    label_padding_value=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jsgfij8WQELq"
   },
   "source": [
    "Теперь всё готово, чтобы задать `DataLoader`'ы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "gFljkiBOQELr"
   },
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=coll)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=128, shuffle=False, collate_fn=coll)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=1,shuffle=False, collate_fn=coll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i34wGJ4uQELr"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "QLlr_DztQELr"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "\n",
    "chars = batch['char_ids'].to(DEVICE)\n",
    "tokens = batch['token_ids'].to(DEVICE)\n",
    "labels = batch['labels'].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[51, 52, 43,  ...,  0,  0,  0],\n          [29,  0,  0,  ...,  0,  0,  0],\n          [13, 52, 45,  ...,  0,  0,  0],\n          ...,\n          [ 0,  0,  0,  ...,  0,  0,  0],\n          [ 0,  0,  0,  ...,  0,  0,  0],\n          [ 0,  0,  0,  ...,  0,  0,  0]],\n \n         [[ 8, 33, 45,  ...,  0,  0,  0],\n          [29,  0,  0,  ...,  0,  0,  0],\n          [55, 33, 51,  ...,  0,  0,  0],\n          ...,\n          [ 0,  0,  0,  ...,  0,  0,  0],\n          [ 0,  0,  0,  ...,  0,  0,  0],\n          [ 0,  0,  0,  ...,  0,  0,  0]],\n \n         [[51, 52, 43,  ...,  0,  0,  0],\n          [29,  0,  0,  ...,  0,  0,  0],\n          [33,  6, 55,  ...,  0,  0,  0],\n          ...,\n          [47,  0,  0,  ...,  0,  0,  0],\n          [26, 29,  2,  ...,  0,  0,  0],\n          [42,  0,  0,  ...,  0,  0,  0]],\n \n         ...,\n \n         [[43, 37, 43,  ...,  0,  0,  0],\n          [29,  0,  0,  ...,  0,  0,  0],\n          [44, 52, 55,  ...,  0,  0,  0],\n          ...,\n          [ 0,  0,  0,  ...,  0,  0,  0],\n          [ 0,  0,  0,  ...,  0,  0,  0],\n          [ 0,  0,  0,  ...,  0,  0,  0]],\n \n         [[ 6, 51, 31,  ...,  0,  0,  0],\n          [32, 55, 52,  ...,  0,  0,  0],\n          [43,  6,  8,  ...,  0,  0,  0],\n          ...,\n          [ 0,  0,  0,  ...,  0,  0,  0],\n          [ 0,  0,  0,  ...,  0,  0,  0],\n          [ 0,  0,  0,  ...,  0,  0,  0]],\n \n         [[36, 33, 45,  ...,  0,  0,  0],\n          [43, 25, 10,  ...,  0,  0,  0],\n          [25, 31, 31,  ...,  0,  0,  0],\n          ...,\n          [ 0,  0,  0,  ...,  0,  0,  0],\n          [ 0,  0,  0,  ...,  0,  0,  0],\n          [ 0,  0,  0,  ...,  0,  0,  0]]], device='cuda:0'),\n torch.Size([8, 506, 20]))"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars, chars.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdMMEDdbQELs",
    "outputId": "b7bd1bb5-248b-482c-cd5e-c98c6c3709f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1515,  570, 8464,  ...,    0,    0,    0],\n         [1635,  570, 1148,  ...,    0,    0,    0],\n         [1515,  570,   15,  ...,  122, 2084,  124],\n         ...,\n         [7803,  570, 1410,  ...,    0,    0,    0],\n         [5754, 3418,    1,  ...,    0,    0,    0],\n         [4923,    1,  963,  ...,    0,    0,    0]], device='cuda:0'),\n torch.Size([8, 506]))"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w--fhADKQELs",
    "outputId": "708d1cec-7e0c-4f07-f723-900aab50203c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0,  0,  1,  ..., -1, -1, -1],\n         [ 0,  0,  0,  ..., -1, -1, -1],\n         [ 0,  0,  2,  ...,  0,  0,  0],\n         ...,\n         [ 0,  0,  0,  ..., -1, -1, -1],\n         [ 3,  0,  0,  ..., -1, -1, -1],\n         [ 4,  8,  0,  ..., -1, -1, -1]], device='cuda:0'),\n torch.Size([8, 506]))"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFeX0AYKlhGk",
    "outputId": "3c5a0193-2caf-4d7c-f02f-272b91e0f6f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Опять же, вид всех тензоров у меня другой, поэтому в этих тестах нет смысла\n"
     ]
    }
   ],
   "source": [
    "# train_tokens, train_labels = next(iter(\n",
    "#     torch.utils.data.DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=2,\n",
    "#         shuffle=False,\n",
    "#         collate_fn=collator,\n",
    "#     )\n",
    "# ))\n",
    "# assert torch.equal(train_tokens, torch.tensor([[ 2,  1,  3,  4,  5,  6,  7,  8,  9], [10, 11,  0,  0,  0,  0,  0,  0,  0]])), \"Похоже на ошибку в коллаторе\"\n",
    "# assert torch.equal(train_labels, torch.tensor([[ 3,  0,  2,  0,  0,  0,  2,  0,  0], [ 4,  8, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "#\n",
    "# valid_tokens, valid_labels = next(iter(\n",
    "#     torch.utils.data.DataLoader(\n",
    "#         valid_dataset,\n",
    "#         batch_size=2,\n",
    "#         shuffle=False,\n",
    "#         collate_fn=collator,\n",
    "#     )\n",
    "# ))\n",
    "# assert torch.equal(valid_tokens, torch.tensor([[ 1737,   571,  1777,   197,   687,   145,   349,   111,  1819,  1558, 9], [  248, 10679,     0,     0,     0,     0,     0,     0,     0,     0,    0]])), \"Похоже на ошибку в коллаторе\"\n",
    "# assert torch.equal(valid_labels, torch.tensor([[ 0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0], [ 1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "#\n",
    "# test_tokens, test_labels = next(iter(\n",
    "#     torch.utils.data.DataLoader(\n",
    "#         test_dataset,\n",
    "#         batch_size=2,\n",
    "#         shuffle=False,\n",
    "#         collate_fn=collator,\n",
    "#     )\n",
    "# ))\n",
    "# assert torch.equal(test_tokens, torch.tensor([[1516,  571, 1434, 1729, 4893, 2014,   67,  310,  215, 3157, 3139,    9], [   1,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])), \"Похоже на ошибку в коллаторе\"\n",
    "# assert torch.equal(test_labels, torch.tensor([[ 0,  0,  1,  0,  0,  0,  0,  4,  0,  0,  0,  0], [ 4,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "\n",
    "print(\"Опять же, вид всех тензоров у меня другой, поэтому в этих тестах нет смысла\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ul5gLriQELs"
   },
   "source": [
    "## Часть 2. BiLSTM-теггер (6 баллов)\n",
    "\n",
    "Определите архитектуру сети, используя библиотеку PyTorch. \n",
    "\n",
    "Ваша архитектура в этом пункте должна соответствовать стандартному теггеру:\n",
    "* Embedding слой на входе\n",
    "* LSTM (однонаправленный или двунаправленный)слой для обработки последовательности\n",
    "* Dropout (заданный отдельно или встроенный в LSTM) для уменьшения переобучения\n",
    "* Linear слой на выходе\n",
    "\n",
    "Для обучения сети используйте поэлементную кросс-энтропийную функцию потерь.\n",
    "\n",
    "**Обратите внимание**, что `<PAD>` токены не должны учавствовать в подсчёте функции потерь. В качестве оптимизатора рекомендуется использовать Adam. Для получения значений предсказаний по выходам модели используйте функцию `argmax`.\n",
    "\n",
    "**Задание. Реализуйте класс модели BiLSTM.** **<font color='red'>(2 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "uMiLQljZQELt"
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from torch import Tensor, BoolTensor\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import Embedding, LSTM, Linear, Dropout, Sequential, Conv1d, Module, ReLU, LayerNorm, Parameter, \\\n",
    "    MaxPool1d, Conv2d, CrossEntropyLoss\n",
    "\n",
    "from torchcrf import CRF\n",
    "\n",
    "\n",
    "class Transpose(Module):\n",
    "\n",
    "    def __init__(self, *dims):\n",
    "        super().__init__()\n",
    "        self._dims = dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.transpose(x, *self._dims)\n",
    "\n",
    "\n",
    "def view_input_tensors_factory(*positional_dims, **keyword_dims):\n",
    "    def view_input_tensors(fn):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return fn(\n",
    "                *(x.view(*dims) for x, dims in zip(args, positional_dims)),\n",
    "                **{name: x.view(*keyword_dims[name]) for name, x in kwargs.items()}\n",
    "            )\n",
    "        return wrapper\n",
    "    return view_input_tensors\n",
    "\n",
    "\n",
    "def add_kwarg_factory(name: str, value: Any):\n",
    "    def add_kwarg(fn):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            return fn(*args, **kwargs, **{name: value})\n",
    "        return wrapper\n",
    "    return add_kwarg\n",
    "\n",
    "\n",
    "def add_mask_to_kwargs_factory(mask_value: Any, build_by: Union[str, int], mask_name: str):\n",
    "    def add_mask_to_kwargs(fn):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if isinstance(build_by, int):\n",
    "                target = args[build_by]\n",
    "            else:\n",
    "                target = kwargs[build_by]\n",
    "            mask = (target != mask_value)\n",
    "            return fn(*args, **kwargs, **{mask_name: mask})\n",
    "        return wrapper\n",
    "    return add_mask_to_kwargs\n",
    "\n",
    "\n",
    "def convert_output_to_tensor_factory(dtype=None, device=None):\n",
    "    def convert_output_to_tensor(fn):\n",
    "        def wrapper(*args, **kwargs) -> Tensor:\n",
    "            return torch.tensor(fn(*args, **kwargs), dtype=dtype, device=device)\n",
    "        return wrapper\n",
    "    return convert_output_to_tensor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AttentionParameters:\n",
    "    dropout: float\n",
    "    dims: int\n",
    "    num_heads: int\n",
    "\n",
    "\n",
    "class SelfAttention(Module):\n",
    "\n",
    "    def __init__(self, attention: Module):\n",
    "        super().__init__()\n",
    "        self._attention = attention\n",
    "\n",
    "    def forward(self, x):\n",
    "        res, _ = self._attention(x, x, x)\n",
    "        return res\n",
    "\n",
    "\n",
    "class ResidualConnection(Module):\n",
    "\n",
    "    def __init__(self, module: Module):\n",
    "        super().__init__()\n",
    "        self._module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        wrapped_res = self._module(x)\n",
    "        return x + wrapped_res\n",
    "\n",
    "\n",
    "class BiLSTM(Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_chars: int,\n",
    "            vocab: Dict[str, int],\n",
    "            char_embedding_dim: int,\n",
    "            pretrained_embeddings: Optional[Path],\n",
    "            freeze_pretrained_embeddings: bool,\n",
    "            max_token_length: int,\n",
    "            token_embedding_dim: int,\n",
    "            hidden_size: int,\n",
    "            num_layers: int,\n",
    "            dropout: float,\n",
    "            interlayer_dropout: float,\n",
    "            bidirectional: bool,\n",
    "            n_classes: int,\n",
    "            char_padding_idx: int,\n",
    "            token_padding_idx: int,\n",
    "            head_type: str,\n",
    "            criterion_type: str,\n",
    "            train_initial_state: bool,\n",
    "            attention_params: Optional[AttentionParameters] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._char_embed = Embedding(num_chars, char_embedding_dim, padding_idx=char_padding_idx)\n",
    "        self._char_conv = Conv2d(1, char_embedding_dim, kernel_size=(3, char_embedding_dim), padding=(1, 0))\n",
    "        self._char_pool = MaxPool1d(kernel_size=max_token_length)\n",
    "        self._char_norm = LayerNorm(char_embedding_dim)\n",
    "        self._char_dropout = Dropout(dropout)\n",
    "\n",
    "        self._padding_idx = token_padding_idx\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            embeddings = {}\n",
    "        else:\n",
    "            embeddings = read_glove(pretrained_embeddings)\n",
    "\n",
    "        self._pretrained_embedding = Embedding(len(embeddings), token_embedding_dim)\n",
    "        self._pretrained_threshold = len(embeddings)\n",
    "\n",
    "        for word in embeddings.keys():\n",
    "            word_idx = vocab[word]\n",
    "            if word_idx >= self._pretrained_threshold:\n",
    "                # the first N words in vocabulary are expected to be pretrained\n",
    "                raise ValueError(f'Malformed vocabulary! Pretrained word {word} has {word_idx} idx while pretrained threshold is {self._pretrained_threshold}!')\n",
    "\n",
    "            word_embedding = embeddings[word]\n",
    "            with torch.no_grad():\n",
    "                self._pretrained_embedding.weight[word_idx] = torch.tensor(word_embedding)\n",
    "\n",
    "        if token_padding_idx < self._pretrained_threshold:\n",
    "            raise ValueError('Padding idx is expected to be pretrained!')\n",
    "\n",
    "        self._true_padding_idx = token_padding_idx - self._pretrained_threshold\n",
    "        self._untrained_embedding = Embedding(len(vocab) - len(embeddings), token_embedding_dim, padding_idx=self._true_padding_idx)\n",
    "\n",
    "        if freeze_pretrained_embeddings:\n",
    "            self._pretrained_embedding.weight.requires_grad = False\n",
    "\n",
    "        embedding_dim = token_embedding_dim + char_embedding_dim\n",
    "        self._embed_dropout = Dropout(dropout)\n",
    "\n",
    "        self._rnn = LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=interlayer_dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        bi = (2 if bidirectional else 1)\n",
    "        self._train_initial_state = train_initial_state\n",
    "        self._h0 = Parameter(torch.empty(bi * num_layers, 1, hidden_size))\n",
    "        self._c0 = Parameter(torch.empty(bi * num_layers, 1, hidden_size))\n",
    "        xavier_uniform_(self._h0.data)\n",
    "        xavier_uniform_(self._c0.data)\n",
    "\n",
    "        lstm_dims = hidden_size * bi\n",
    "\n",
    "        if head_type == 'linear':\n",
    "            self._head = Sequential(\n",
    "                Dropout(dropout),\n",
    "                Linear(lstm_dims, n_classes)\n",
    "            )\n",
    "        elif head_type == 'conv':\n",
    "            self._head = Sequential(\n",
    "                LayerNorm(lstm_dims),\n",
    "                Dropout(dropout),\n",
    "                Transpose(-2, -1),\n",
    "                Conv1d(lstm_dims, n_classes, kernel_size=3, padding=1),\n",
    "                Transpose(-2, -1)\n",
    "            )\n",
    "        elif head_type == 'ff':\n",
    "            self._head = Sequential(\n",
    "                LayerNorm(lstm_dims),\n",
    "                Dropout(dropout),\n",
    "                Linear(lstm_dims, lstm_dims // 4),\n",
    "                LayerNorm(lstm_dims // 4),\n",
    "                ReLU(inplace=True),\n",
    "                Dropout(dropout),\n",
    "                Linear(lstm_dims // 4, lstm_dims // 16),\n",
    "                LayerNorm(lstm_dims // 16),\n",
    "                ReLU(inplace=True),\n",
    "                Dropout(dropout),\n",
    "                Linear(lstm_dims // 16, n_classes)\n",
    "            )\n",
    "        elif head_type == 'attention':\n",
    "            if attention_params is None:\n",
    "                raise ValueError\n",
    "\n",
    "            self._head = Sequential(  # similar to transformer encoder block\n",
    "                Dropout(dropout),\n",
    "                Linear(lstm_dims, attention_params.dims),\n",
    "                ReLU(inplace=True),\n",
    "                LayerNorm(attention_params.dims),\n",
    "                ResidualConnection(SelfAttention(torch.nn.MultiheadAttention(\n",
    "                    embed_dim=attention_params.dims,\n",
    "                    num_heads=attention_params.num_heads,\n",
    "                    dropout=attention_params.dropout,\n",
    "                    batch_first=True\n",
    "                ))),\n",
    "                Linear(attention_params.dims, n_classes)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        if criterion_type == 'celoss':\n",
    "            self._celoss = CrossEntropyLoss(ignore_index=-1)\n",
    "            self._criterion = view_input_tensors_factory((-1, n_classes), (-1,))(self._celoss)\n",
    "            self._decoder: Callable[[Tensor], LongTensor] = partial(torch.argmax, dim=-1)\n",
    "            self._loss_postprocess = lambda x: x\n",
    "        elif criterion_type == 'crf':\n",
    "            self._crf = CRF(n_classes, batch_first=True)\n",
    "            self._criterion = add_kwarg_factory(name='reduction', value='token_mean')(add_mask_to_kwargs_factory(mask_value=-1, build_by=1, mask_name='mask')(self._crf))\n",
    "            self._decoder = convert_output_to_tensor_factory(dtype=torch.long, device=DEVICE)(self._crf.decode)\n",
    "            self._loss_postprocess = lambda x: -x  # crf returns log-likelihood\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def _embed_tokens(self, token_ids: LongTensor) -> Tensor:\n",
    "        token_repr = self._untrained_embedding(torch.full_like(token_ids, fill_value=self._true_padding_idx))\n",
    "        pretrained_mask = (token_ids < self._pretrained_threshold)\n",
    "        untrained_mask = ~pretrained_mask\n",
    "        token_repr[pretrained_mask] = self._pretrained_embedding(token_ids[pretrained_mask])\n",
    "        token_repr[untrained_mask] = self._untrained_embedding(token_ids[untrained_mask] - self._pretrained_threshold)\n",
    "\n",
    "        return token_repr\n",
    "\n",
    "\n",
    "    def _embed_chars(self, char_ids: LongTensor) -> Tensor:\n",
    "        char_repr = self._char_embed(char_ids)  # (B, SL, TL, CH)\n",
    "        char_repr = self._char_dropout(self._char_norm(char_repr))\n",
    "        batch_size, sequence_length, token_length, char_embed_dims = char_repr.shape\n",
    "        char_repr = self._char_conv(char_repr.view(-1, 1, token_length, char_embed_dims)).squeeze(-1)  # (B * SL, CH, TL)\n",
    "        char_repr = self._char_pool(char_repr).squeeze(-1).view(batch_size, sequence_length, char_embed_dims)\n",
    "\n",
    "        return char_repr\n",
    "\n",
    "\n",
    "    def _lstm_pass(self, features: Tensor, padding_mask: BoolTensor) -> Tensor:\n",
    "        # используем специальную функцию pack_padded_sequence для того, чтобы получить структуру PackedSequence\n",
    "        # которая не учитывать паддинг при проходе rnn\n",
    "        length = padding_mask.sum(dim=1).detach().cpu()\n",
    "        packed_embed = torch.nn.utils.rnn.pack_padded_sequence(features, length, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # используем специальную функцию pad_packed_sequence для того, чтобы получить тензор из PackedSequence\n",
    "        if not self._train_initial_state:\n",
    "            packed_rnn_output, _ = self._rnn(packed_embed)\n",
    "        else:\n",
    "            batch_size = features.shape[0]\n",
    "            packed_rnn_output, _ = self._rnn(packed_embed, (self._h0.repeat(1, batch_size, 1), self._c0.repeat(1, batch_size, 1)))\n",
    "        rnn_output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_rnn_output, batch_first=True)\n",
    "        return rnn_output\n",
    "\n",
    "\n",
    "    def forward(self, char_ids: LongTensor, token_ids: LongTensor, labels: Optional[LongTensor] = None, return_predictions: bool = False, **_) -> Union[Tensor, Tuple[Tensor, LongTensor]]:\n",
    "        embed = self._embed_dropout(torch.concat([self._embed_tokens(token_ids), self._embed_chars(char_ids)], dim=-1))\n",
    "        rnn_output = self._lstm_pass(embed, (token_ids != self._padding_idx))\n",
    "\n",
    "        label_scores = self._head(rnn_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self._loss_postprocess(self._criterion(label_scores, labels))\n",
    "            if return_predictions:\n",
    "                return loss, self._decoder(label_scores)\n",
    "            return loss\n",
    "\n",
    "        return self._decoder(label_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "'1.13.0+cu117'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "zps8HL2VQELu"
   },
   "outputs": [],
   "source": [
    "m = BiLSTM(\n",
    "    num_chars=len(c2idx),\n",
    "    vocab=t2idx,\n",
    "    char_embedding_dim=50,\n",
    "    token_embedding_dim=100,\n",
    "    pretrained_embeddings=None,\n",
    "    hidden_size=100,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    interlayer_dropout=0.0,\n",
    "    bidirectional=True,\n",
    "    n_classes=len(label2idx),\n",
    "    char_padding_idx=c2idx['<PAD>'],\n",
    "    token_padding_idx=t2idx['<PAD>'],\n",
    "    max_token_length=20,\n",
    "    head_type='ff',\n",
    "    criterion_type='crf',\n",
    "    train_initial_state=True,\n",
    "    freeze_pretrained_embeddings=True\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmg2_C_oQELu",
    "outputId": "cc700776-3bbe-43a3-d0db-049ad74965e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "BiLSTM(\n  (_char_embed): Embedding(60, 50, padding_idx=0)\n  (_char_conv): Conv2d(1, 50, kernel_size=(3, 50), stride=(1, 1), padding=(1, 0))\n  (_char_pool): MaxPool1d(kernel_size=20, stride=20, padding=0, dilation=1, ceil_mode=False)\n  (_char_norm): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n  (_char_dropout): Dropout(p=0.0, inplace=False)\n  (_pretrained_embedding): Embedding(0, 100)\n  (_untrained_embedding): Embedding(10951, 100, padding_idx=0)\n  (_embed_dropout): Dropout(p=0.0, inplace=False)\n  (_rnn): LSTM(150, 100, batch_first=True, bidirectional=True)\n  (_head): Sequential(\n    (0): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n    (1): Dropout(p=0.0, inplace=False)\n    (2): Linear(in_features=200, out_features=50, bias=True)\n    (3): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.0, inplace=False)\n    (6): Linear(in_features=50, out_features=12, bias=True)\n    (7): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n    (8): ReLU(inplace=True)\n    (9): Dropout(p=0.0, inplace=False)\n    (10): Linear(in_features=12, out_features=9, bias=True)\n  )\n  (_crf): CRF(num_tags=9)\n)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "sDvWB5J2QELv"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Jn5Pu1UKQELv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor-sch/dl-2022/venv/lib/python3.8/site-packages/torchcrf/__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([8, 506]), tensor(2.7243, device='cuda:0', grad_fn=<NegBackward0>))"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, pred = m(chars, tokens, labels, return_predictions=True)\n",
    "pred.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n02Bsh8eQELw",
    "outputId": "6b704a7d-2cc9-49b0-a260-d4f6f52d2405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert pred.shape == torch.Size([8, 506])\n",
    "assert 2 < loss < 3\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjhkK9QFQELu"
   },
   "source": [
    "### Эксперименты\n",
    "\n",
    "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1-macro мере на валидационной и тестовой выборках было не меньше 0.76. \n",
    "\n",
    "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "f4hdrFZ9iRPi"
   },
   "outputs": [],
   "source": [
    "# создадим SummaryWriter для эксперимента с BiLSTMModel\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"logs/BiLSTMModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruMTBSkQQELx"
   },
   "source": [
    "**Задание. Реализуйте функцию подсчета метрик compute_metrics.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "xkpo3JgWQELx"
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Entity:\n",
    "    start: int\n",
    "    end: int\n",
    "    type: str\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.start, self.end, self.type))\n",
    "\n",
    "\n",
    "def ent_type(lbl: str) -> str:\n",
    "    return lbl.split('-')[1]\n",
    "\n",
    "\n",
    "def is_ent(lbl: str) -> bool:\n",
    "    return lbl.split('-')[0] in {'B', 'I'}\n",
    "\n",
    "\n",
    "def is_ent_start(lbl: str) -> bool:\n",
    "    return lbl.startswith('B')\n",
    "\n",
    "\n",
    "# TODO: rewrite only for entity labels + their positions\n",
    "def decode_labels(labels: Iterable[str]) -> Iterable[Entity]:\n",
    "\n",
    "    prev_label = 'O'\n",
    "    start: Optional[int] = None\n",
    "\n",
    "    curr_idx = 0\n",
    "    for curr_idx, label in enumerate(labels):\n",
    "        if is_ent(label):\n",
    "            if is_ent(prev_label):\n",
    "                # ... X1-ENT1 X2-ENT2 ...\n",
    "                if ent_type(label) != ent_type(prev_label) or is_ent_start(label):\n",
    "                    # ENT1 != ENT2 or X2 == B - entity ended + start new entity\n",
    "                    yield Entity(start, curr_idx, ent_type(prev_label))\n",
    "                    start = curr_idx\n",
    "                # X2 != B and ENT1 == ENT2 - do nothing\n",
    "            else:\n",
    "                # ... O X-ENT ... - start new entity\n",
    "                start = curr_idx\n",
    "        else:\n",
    "            if is_ent(prev_label):\n",
    "                # ... X-ENT O ... - entity ended\n",
    "                yield Entity(start, curr_idx, ent_type(prev_label))\n",
    "                start = None  # do not start new entity\n",
    "            # ... O O ... - do nothing\n",
    "\n",
    "        prev_label = label\n",
    "\n",
    "    if is_ent(prev_label):\n",
    "        # sequence ended on entity\n",
    "        yield Entity(start, curr_idx + 1, ent_type(prev_label))\n",
    "\n",
    "\n",
    "def safe(f: Callable[[], Any], default: Any = None) -> Any:\n",
    "    try:\n",
    "        return f()\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "\n",
    "def compute_metrics(preds: LongTensor, label_ids: LongTensor, label_mapping: np.ndarray, ignore_index: int = -1) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute NER metrics. All input tensors should be ravelled.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {}\n",
    "    label_mask = (label_ids != ignore_index)\n",
    "    predicted_label_ids = preds[label_mask].detach().cpu().numpy()\n",
    "    gold_label_ids = label_ids[label_mask].detach().cpu().numpy()\n",
    "\n",
    "    predicted_labels = label_mapping[predicted_label_ids]\n",
    "    gold_labels = label_mapping[gold_label_ids]\n",
    "\n",
    "    predicted_entities = set(decode_labels(predicted_labels))\n",
    "    gold_entities = set(decode_labels(gold_labels))\n",
    "\n",
    "    true_positive = predicted_entities.intersection(gold_entities)\n",
    "    false_positive = predicted_entities.difference(gold_entities)\n",
    "    false_negative = gold_entities.difference(predicted_entities)\n",
    "\n",
    "    recall_micro = safe(lambda: len(true_positive) / (len(true_positive) + len(false_negative)), 0.0)\n",
    "    precision_micro = safe(lambda: len(true_positive) / (len(true_positive) + len(false_positive)), 0.0)\n",
    "    f1_micro = safe(lambda: 2 * precision_micro * recall_micro / (precision_micro + recall_micro), 0.0)\n",
    "\n",
    "    all_f1 = []\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_weights = []\n",
    "\n",
    "    for label in label_mapping:\n",
    "        if not is_ent_start(label):\n",
    "            continue\n",
    "\n",
    "        target_type = ent_type(label)\n",
    "        predicate = lambda entity: entity.type == target_type\n",
    "\n",
    "        target_predicted_entities = set(filter(predicate, predicted_entities))\n",
    "        target_gold_entities = set(filter(predicate, gold_entities))\n",
    "\n",
    "        true_positive = target_predicted_entities.intersection(target_gold_entities)\n",
    "        false_positive = target_predicted_entities.difference(target_gold_entities)\n",
    "        false_negative = target_gold_entities.difference(predicted_entities)\n",
    "\n",
    "        recall = safe(lambda: len(true_positive) / (len(true_positive) + len(false_negative)), 0.0)\n",
    "        precision = safe(lambda: len(true_positive) / (len(true_positive) + len(false_positive)), 0.0)\n",
    "        f1 = safe(lambda: 2 * precision * recall / (precision + recall), 0.0)\n",
    "\n",
    "        metrics[f'recall_{target_type}'] = recall\n",
    "        metrics[f'precision_{target_type}'] = precision\n",
    "        metrics[f'f1_{target_type}'] = f1\n",
    "\n",
    "        all_f1.append(f1)\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "        all_weights.append(len(target_gold_entities) / len(gold_entities))\n",
    "\n",
    "    all_f1 = np.array(all_f1)\n",
    "    all_recall = np.array(all_recall)\n",
    "    all_precision = np.array(all_precision)\n",
    "\n",
    "    recall_macro = all_recall.mean()\n",
    "    precision_macro = all_precision.mean()\n",
    "    f1_macro = all_f1.mean()\n",
    "\n",
    "    recall_weighted = (all_recall * all_weights).sum()\n",
    "    precision_weighted = (all_precision * all_weights).sum()\n",
    "    f1_weighted = (all_f1 * all_weights).sum()\n",
    "\n",
    "    metrics[\"precision_micro\"] = precision_micro\n",
    "    metrics[\"precision_macro\"] = precision_macro\n",
    "    metrics[\"precision_weighted\"] = precision_weighted\n",
    "\n",
    "    metrics[\"recall_micro\"] = recall_micro\n",
    "    metrics[\"recall_macro\"] = recall_macro\n",
    "    metrics[\"recall_weighted\"] = recall_weighted\n",
    "\n",
    "    metrics[\"f1_micro\"] = f1_micro\n",
    "    metrics[\"f1_macro\"] = f1_macro\n",
    "    metrics[\"f1_weighted\"] = f1_weighted\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzj89UygQEL0"
   },
   "source": [
    "**Задание. Реализуйте функции обучения и тестирования train_epoch и evaluate_epoch.** **<font color='red'>(2 балла)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "sG3vQbc_QEL0"
   },
   "outputs": [],
   "source": [
    "from transformers import get_constant_schedule_with_warmup, get_linear_schedule_with_warmup, \\\n",
    "    get_cosine_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler\n",
    "from copy import deepcopy\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn import Module\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def validate(model: Module, dataloader: DataLoader, label_mapping: np.ndarray, ignore_index: int) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    # collect all revelled predictions\n",
    "\n",
    "    all_predictions: List[Tensor] = []\n",
    "    all_label_ids: List[LongTensor] = []\n",
    "    all_losses: List[float] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            label_ids = batch['labels']\n",
    "\n",
    "            loss, predictions = model(**batch, return_predictions=True)\n",
    "\n",
    "            # ravel batch and length dims\n",
    "\n",
    "            ravelled_predictions = predictions.detach().cpu().reshape(-1)\n",
    "            ravelled_label_ids = label_ids.detach().cpu().reshape(-1)\n",
    "\n",
    "            all_predictions.append(ravelled_predictions)\n",
    "            all_label_ids.append(ravelled_label_ids)\n",
    "            all_losses.append(loss.item())\n",
    "\n",
    "        combined_predictions = torch.concat(all_predictions)\n",
    "        combined_label_ids = torch.concat(all_label_ids)\n",
    "\n",
    "    mean_loss = sum(all_losses) / len(all_losses)\n",
    "    return {**compute_metrics(combined_predictions, combined_label_ids, label_mapping, ignore_index), 'loss': mean_loss}\n",
    "\n",
    "\n",
    "def log_metrics(metrics: Dict[str, float], writer: SummaryWriter, *, step: int) -> None:\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        writer.add_scalar(metric_name, metric_value, global_step=step)\n",
    "\n",
    "\n",
    "def get_state(model: Module) -> dict:\n",
    "    model.cpu()\n",
    "    state = deepcopy(model.state_dict())\n",
    "    model.to(DEVICE)\n",
    "    return state\n",
    "\n",
    "\n",
    "def set_state(model: Module, state: dict) -> Module:\n",
    "    model.cpu()\n",
    "    model.load_state_dict(state)\n",
    "    model.to(DEVICE)\n",
    "    return model\n",
    "\n",
    "\n",
    "def set_lr(optimizer: Optimizer, lr: float) -> None:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "        model: Module,\n",
    "        train_dataloader: DataLoader,\n",
    "        val_dataloader: DataLoader,\n",
    "        label_mapping: np.ndarray,\n",
    "        optimizer: Optimizer,\n",
    "        batch_size: int,\n",
    "        lr_schedule: str,\n",
    "        initial_lr: float,\n",
    "        drop_factor_lr: float,\n",
    "        min_lr: float,\n",
    "        max_epochs: int,\n",
    "        reload_patience: int,\n",
    "        clip_grads: bool,\n",
    "        max_grad_norm: float,\n",
    "        warmup_epochs: float,\n",
    "        val_period: int,\n",
    "        disable_amp: bool,\n",
    "        target_metric: str,\n",
    "        ignore_index: int,\n",
    "        writer: SummaryWriter\n",
    ") -> Module:\n",
    "    \"\"\"\n",
    "    One training cycle (loop).\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch = 0\n",
    "    epoch_examples = len(train_dataloader) * batch_size\n",
    "    examples_seen = 0\n",
    "\n",
    "    first_time = True\n",
    "    examples_from_last_log = 0\n",
    "    logging_loss_sum = 0\n",
    "\n",
    "    curr_lr = 0 if warmup_epochs > 0.0 else initial_lr\n",
    "    curr_patience = reload_patience\n",
    "\n",
    "    if lr_schedule == 'constant':\n",
    "        scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=int(epoch_examples * warmup_epochs / batch_size))\n",
    "    elif lr_schedule == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(epoch_examples * warmup_epochs / batch_size),\n",
    "            num_training_steps=int(epoch_examples * max_epochs / batch_size)\n",
    "        )\n",
    "    elif lr_schedule == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(epoch_examples * warmup_epochs / batch_size),\n",
    "            num_training_steps=int(epoch_examples * max_epochs / batch_size)\n",
    "        )\n",
    "    elif lr_schedule == 'adaptive':\n",
    "        scheduler = DummyScheduler()\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    best_model_state = get_state(model)\n",
    "    best_metric = np.nan\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    while (examples_seen <= epoch_examples * warmup_epochs or curr_lr > min_lr) and not epoch > max_epochs:\n",
    "        epoch += 1\n",
    "        print(f'{epoch} epoch, best metric: {best_metric:.3f}')\n",
    "        for batch in train_dataloader:\n",
    "            model.train()\n",
    "\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            label_ids = batch['labels']\n",
    "\n",
    "            if disable_amp:\n",
    "                optimizer.zero_grad()\n",
    "                loss = model(**batch)\n",
    "                loss.backward()\n",
    "                if clip_grads:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss = model(**batch)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                if clip_grads:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            scheduler.step()\n",
    "            curr_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            examples_seen += len(label_ids)\n",
    "\n",
    "            logging_loss_sum += loss.item() * len(label_ids)\n",
    "            examples_from_last_log += len(label_ids)\n",
    "\n",
    "            if examples_from_last_log >= val_period:\n",
    "                metrics = validate(model, val_dataloader, label_mapping, ignore_index)\n",
    "\n",
    "                if examples_seen >= epoch_examples * warmup_epochs:\n",
    "                    if first_time:\n",
    "                        curr_lr = initial_lr\n",
    "                        first_time = False\n",
    "                        set_lr(optimizer, curr_lr)\n",
    "\n",
    "                    target_metric_value = metrics[target_metric]\n",
    "                    if best_metric is np.nan or target_metric_value > best_metric:\n",
    "                        best_model_state = get_state(model)\n",
    "                        best_metric = target_metric_value\n",
    "                        curr_patience = reload_patience\n",
    "                    else:\n",
    "                        curr_patience -= 1\n",
    "                        if curr_patience <= 0:\n",
    "                            set_state(model, best_model_state)\n",
    "                            curr_patience = reload_patience\n",
    "\n",
    "                            if lr_schedule == 'adaptive':\n",
    "                                curr_lr *= drop_factor_lr\n",
    "                                set_lr(optimizer, curr_lr)\n",
    "                else:\n",
    "                    if lr_schedule == 'adaptive':\n",
    "                        # warmup period\n",
    "                        curr_lr = initial_lr * examples_seen / (epoch_examples * warmup_epochs)\n",
    "                        set_lr(optimizer, curr_lr)\n",
    "\n",
    "                metrics['train_loss'] = logging_loss_sum / examples_from_last_log\n",
    "                examples_from_last_log = 0\n",
    "                logging_loss_sum = 0\n",
    "\n",
    "                metrics['learning_rate'] = curr_lr\n",
    "                metrics['epoch'] = examples_seen / epoch_examples\n",
    "\n",
    "                log_metrics(metrics, writer, step=examples_seen)\n",
    "\n",
    "    set_state(model, best_model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTxfU0BfQEL1"
   },
   "source": [
    "**Задание. Проведите эксперименты.** **<font color='red'>(2 балла)</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "!rm -rd logs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "label_mapping = {v: k for k, v in label2idx.items()}\n",
    "label_mapping = np.array([label_mapping[idx] for idx in range(len(label_mapping))])\n",
    "\n",
    "exp_id = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 946 documents and 14041 sentences from conll03/data/train.txt\n",
      "Read 216 documents and 3250 sentences from conll03/data/valid.txt\n",
      "Read 231 documents and 3453 sentences from conll03/data/test.txt\n",
      "Building pretrained vocabulary from glove/data/glove.6B.300d.txt\n",
      "Vocabulary size: 400000\n",
      "Final vocabulary size: 400023\n"
     ]
    }
   ],
   "source": [
    "embeddings_path = Path('glove/data/glove.6B.300d.txt')\n",
    "\n",
    "convert_to_iob = True\n",
    "train_docs = read_conll2003(Path('conll03/data/train.txt'), convert_to_iob=convert_to_iob, lower=False)\n",
    "val_docs = read_conll2003(Path('conll03/data/valid.txt'), convert_to_iob=convert_to_iob, lower=False)\n",
    "test_docs = read_conll2003(Path('conll03/data/test.txt'), convert_to_iob=convert_to_iob, lower=False)\n",
    "\n",
    "lower = True\n",
    "char2idx = get_char2idx(token2cnt.keys(), num_embedding=False)\n",
    "token2cnt = get_token_counts(train_docs, lower=lower)\n",
    "token2idx = get_token2idx(token2cnt, min_count=10, base_dict=glove2vocab(embeddings_path))\n",
    "\n",
    "collator = NERCollator(\n",
    "    char_padding_value=c2idx['<PAD>'],\n",
    "    token_padding_value=t2idx['<PAD>'],\n",
    "    label_padding_value=-1,\n",
    ")\n",
    "\n",
    "max_token_length = 20\n",
    "max_sequence_length = 100000000000000000000\n",
    "\n",
    "train_dataset = NERDataset(\n",
    "    train_docs,\n",
    "    char2idx=char2idx, token2idx=token2idx, label2idx=label2idx,\n",
    "    max_token_length=max_token_length, max_sequence_length=max_sequence_length,\n",
    "    lower=lower\n",
    ")\n",
    "valid_dataset = NERDataset(\n",
    "    val_docs,\n",
    "    char2idx=char2idx, token2idx=token2idx, label2idx=label2idx,\n",
    "    max_token_length=max_token_length, max_sequence_length=max_sequence_length,\n",
    "    lower=lower\n",
    ")\n",
    "test_dataset = NERDataset(\n",
    "    test_docs,\n",
    "    char2idx=char2idx, token2idx=token2idx, label2idx=label2idx,\n",
    "    max_token_length=max_token_length, max_sequence_length=max_sequence_length,\n",
    "    lower=lower\n",
    ")\n",
    "\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 512\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collator)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=eval_batch_size, shuffle=False, collate_fn=collator)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=eval_batch_size,shuffle=False, collate_fn=collator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "yz6mjGZUQEL2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting word embeddings from glove/data/glove.6B.300d.txt\n",
      "Extracted 400000 embeddings\n",
      "1 epoch, best metric: nan\n",
      "2 epoch, best metric: 0.148\n",
      "3 epoch, best metric: 0.749\n",
      "4 epoch, best metric: 0.842\n",
      "5 epoch, best metric: 0.865\n",
      "6 epoch, best metric: 0.892\n",
      "7 epoch, best metric: 0.900\n",
      "8 epoch, best metric: 0.900\n",
      "9 epoch, best metric: 0.903\n",
      "10 epoch, best metric: 0.914\n",
      "11 epoch, best metric: 0.916\n",
      "12 epoch, best metric: 0.916\n",
      "13 epoch, best metric: 0.916\n",
      "14 epoch, best metric: 0.922\n",
      "15 epoch, best metric: 0.922\n",
      "16 epoch, best metric: 0.922\n",
      "17 epoch, best metric: 0.922\n",
      "18 epoch, best metric: 0.922\n",
      "19 epoch, best metric: 0.923\n",
      "20 epoch, best metric: 0.923\n",
      "21 epoch, best metric: 0.926\n",
      "22 epoch, best metric: 0.926\n",
      "23 epoch, best metric: 0.926\n",
      "24 epoch, best metric: 0.926\n",
      "25 epoch, best metric: 0.926\n",
      "26 epoch, best metric: 0.926\n",
      "27 epoch, best metric: 0.926\n",
      "28 epoch, best metric: 0.926\n",
      "29 epoch, best metric: 0.926\n",
      "30 epoch, best metric: 0.926\n",
      "31 epoch, best metric: 0.927\n",
      "32 epoch, best metric: 0.927\n",
      "33 epoch, best metric: 0.927\n",
      "34 epoch, best metric: 0.927\n",
      "35 epoch, best metric: 0.927\n",
      "36 epoch, best metric: 0.927\n",
      "37 epoch, best metric: 0.927\n",
      "38 epoch, best metric: 0.927\n",
      "39 epoch, best metric: 0.927\n",
      "40 epoch, best metric: 0.927\n",
      "41 epoch, best metric: 0.927\n",
      "42 epoch, best metric: 0.927\n",
      "43 epoch, best metric: 0.927\n",
      "44 epoch, best metric: 0.927\n",
      "45 epoch, best metric: 0.928\n",
      "46 epoch, best metric: 0.928\n",
      "47 epoch, best metric: 0.928\n",
      "48 epoch, best metric: 0.928\n",
      "49 epoch, best metric: 0.929\n",
      "50 epoch, best metric: 0.929\n",
      "51 epoch, best metric: 0.929\n",
      "52 epoch, best metric: 0.929\n",
      "53 epoch, best metric: 0.929\n",
      "54 epoch, best metric: 0.929\n",
      "55 epoch, best metric: 0.929\n",
      "56 epoch, best metric: 0.929\n",
      "57 epoch, best metric: 0.929\n",
      "58 epoch, best metric: 0.929\n",
      "59 epoch, best metric: 0.929\n",
      "60 epoch, best metric: 0.929\n",
      "61 epoch, best metric: 0.929\n",
      "62 epoch, best metric: 0.929\n",
      "63 epoch, best metric: 0.929\n",
      "64 epoch, best metric: 0.929\n",
      "65 epoch, best metric: 0.929\n",
      "66 epoch, best metric: 0.929\n",
      "67 epoch, best metric: 0.929\n",
      "68 epoch, best metric: 0.929\n",
      "69 epoch, best metric: 0.929\n",
      "70 epoch, best metric: 0.929\n",
      "71 epoch, best metric: 0.929\n",
      "72 epoch, best metric: 0.929\n",
      "73 epoch, best metric: 0.929\n",
      "74 epoch, best metric: 0.929\n",
      "75 epoch, best metric: 0.929\n",
      "76 epoch, best metric: 0.929\n",
      "77 epoch, best metric: 0.929\n",
      "78 epoch, best metric: 0.929\n",
      "79 epoch, best metric: 0.929\n",
      "80 epoch, best metric: 0.929\n",
      "81 epoch, best metric: 0.929\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "exp_id += 1\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"logs/BiLSTMModel_{exp_id}\")\n",
    "\n",
    "lr = 0.015\n",
    "model = BiLSTM(\n",
    "    num_chars=len(char2idx),\n",
    "    vocab=token2idx,\n",
    "    char_embedding_dim=50,\n",
    "    token_embedding_dim=300,\n",
    "    pretrained_embeddings=embeddings_path,\n",
    "    freeze_pretrained_embeddings=True,\n",
    "    hidden_size=500,\n",
    "    num_layers=1,\n",
    "    dropout=0.3,\n",
    "    interlayer_dropout=0.0,\n",
    "    bidirectional=True,\n",
    "    n_classes=len(label2idx),\n",
    "    max_token_length=max_token_length,\n",
    "    char_padding_idx=char2idx['<PAD>'],\n",
    "    token_padding_idx=token2idx['<PAD>'],\n",
    "    head_type='linear',\n",
    "    criterion_type='crf',\n",
    "    train_initial_state=True\n",
    ").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = train_loop(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=valid_dataloader,\n",
    "    label_mapping=label_mapping,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=train_batch_size,\n",
    "    initial_lr=lr,\n",
    "    drop_factor_lr=0.5,\n",
    "    min_lr=1e-4,\n",
    "    reload_patience=10,\n",
    "    lr_schedule='adaptive',\n",
    "    max_epochs=2000,\n",
    "    clip_grads=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_epochs=0.0,\n",
    "    val_period=500,\n",
    "    disable_amp=False,\n",
    "    target_metric='f1_macro',\n",
    "    ignore_index=-1,\n",
    "    writer=writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_LOC': 0.9653856636685746,\n",
      " 'f1_MISC': 0.8841870824053452,\n",
      " 'f1_ORG': 0.903731343283582,\n",
      " 'f1_PER': 0.9636264929424538,\n",
      " 'f1_macro': 0.9292326455749889,\n",
      " 'f1_micro': 0.9385408741229182,\n",
      " 'f1_weighted': 0.9383252026780184,\n",
      " 'loss': 0.04128353297710419,\n",
      " 'precision_LOC': 0.9630233822729745,\n",
      " 'precision_MISC': 0.9002267573696145,\n",
      " 'precision_ORG': 0.9044062733383121,\n",
      " 'precision_PER': 0.9584233261339092,\n",
      " 'precision_macro': 0.9315199347787027,\n",
      " 'precision_micro': 0.9389377537212449,\n",
      " 'precision_weighted': 0.938614228801651,\n",
      " 'recall_LOC': 0.9677595628415301,\n",
      " 'recall_MISC': 0.8687089715536105,\n",
      " 'recall_ORG': 0.9030574198359433,\n",
      " 'recall_PER': 0.9688864628820961,\n",
      " 'recall_macro': 0.927103104278295,\n",
      " 'recall_micro': 0.9381443298969072,\n",
      " 'recall_weighted': 0.9381443298969072}\n"
     ]
    }
   ],
   "source": [
    "metrics = validate(model, valid_dataloader, label_mapping, -1)\n",
    "pprint(metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_LOC': 0.9264926492649265,\n",
      " 'f1_MISC': 0.7897435897435897,\n",
      " 'f1_ORG': 0.8688029020556227,\n",
      " 'f1_PER': 0.9534520462355514,\n",
      " 'f1_macro': 0.8846227968249226,\n",
      " 'f1_micro': 0.9005086106897475,\n",
      " 'f1_weighted': 0.9001950797436944,\n",
      " 'loss': 0.09486447274684906,\n",
      " 'precision_LOC': 0.9262147570485902,\n",
      " 'precision_MISC': 0.8117469879518072,\n",
      " 'precision_ORG': 0.8651414810355208,\n",
      " 'precision_PER': 0.9543464665415885,\n",
      " 'precision_macro': 0.8893624231443767,\n",
      " 'precision_micro': 0.9025219102128421,\n",
      " 'precision_weighted': 0.9020405061364487,\n",
      " 'recall_LOC': 0.9267707082833133,\n",
      " 'recall_MISC': 0.7689015691868759,\n",
      " 'recall_ORG': 0.8724954462659381,\n",
      " 'recall_PER': 0.9525593008739076,\n",
      " 'recall_macro': 0.8801817561525087,\n",
      " 'recall_micro': 0.8985042735042735,\n",
      " 'recall_weighted': 0.8985042735042735}\n"
     ]
    }
   ],
   "source": [
    "metrics = validate(model, test_dataloader, label_mapping, -1)\n",
    "pprint(metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вообще с этой архитектурой можно выбить 0.9 на тесте, но мне надоело.\n",
    "\n",
    "Влияние контекста (f1_macro на тесте):\n",
    "\n",
    "| 1 пример = 1 предложение | 64     | 128    | 256    | 512    | 1 пример = 1 документ |\n",
    "|--------------------------|--------|--------|--------|--------|-----------------------|\n",
    "| 0.8709                   | 0.8726 | 0.8811 | 0.8815 | 0.8857 | 0.8846                |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8R6nopyQEL-"
   },
   "source": [
    "## Часть 3. Transformers-теггер (6 баллов)\n",
    "\n",
    "В данной части задания нужно сделать все то же самое, но с использованием модели на базе архитектуры Transformer, а именно предлагается дообучать предобученную модель **BERT**.\n",
    "\n",
    "Для данной модели подразумевается специальная подготовка данных, с чего мы и начнем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrbX5gFDQEL-"
   },
   "source": [
    "Модель **BERT** использует специальный токенизатор WordPiece для разбиения предложений на токены. Готовая предобученная версия такого токенизатора существует в библиотеке **transformers**. Есть два класса: `BertTokenizer` и `BertTokenizerFast`. Использовать можно любой, но второй вариант работает существенно быстрее.\n",
    "\n",
    "Токенизаторы можно обучать с нуля на своем корпусе данных, а можно подгружать уже готовые. Готовые токенизаторы, как правило, соответствуют предобученной конфигурации модели, которая использует словарь из этого токенизатора. \n",
    "\n",
    "Мы будем использовать базовую конфигурацию предобученного **BERT** для модели и токенизатора.\n",
    "\n",
    "P.S. Часто приходится проводить эксперименты с моделями разной архитектуры, например **BERT** и **GPT**, поэтому удобно использовать класс `AutoTokenizer`, который по названию модели сам определит, какой класс нужен для инициализации токенизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "4-UTiI4gQEL-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "kSbBhvnDQEMA"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxWNX5i6QEMA"
   },
   "source": [
    "Подгружение предобученных моделей и токенизаторов в **huggingface** происходит с помощью конструктора **from_pretrained**.\n",
    "\n",
    "В данном конструкторе можно указать либо путь к предобученному токенизатору, либо название предобученной конфигурации, как в нашем случае: тогда **transformers** сам подгрузит нужные параметры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "3tg_bCeaQEMA"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MIrbmNoQEMA"
   },
   "source": [
    "### Подготовка словарей\n",
    "\n",
    "В сравнении с рекуррентными моделями, на больше не нужно заниматься сборкой словаря, так как это уже сделано заранее благодаря токенизаторам и алгоритмам, стоящими за ними.\n",
    "\n",
    "Но нам как и прежде потребуется:\n",
    "- {**label**}→{**label_idx**}: соответствие между тегом и уникальным индексом (начинается с 0);\n",
    "\n",
    "Но данное отображение у нас уже реализовано в одной из предыдущих частей задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvYF-4uaQEMB"
   },
   "source": [
    "### Подготовка датасета и загрузчика\n",
    "\n",
    "Мы также хотим обучать модель батчами, поэтому нам как и прежде понадобятся `Dataset`, `Collator` и `DataLoader`.\n",
    "\n",
    "Но мы не можем переиспользовать те, что в предыдущих частях задания, так как обработка данных должна производится немного иначе с использованием токенизатора.\n",
    "\n",
    "Давайте напишем новый кастомный датасет, который на вход (метод `__init__`) будет принимать:\n",
    "- token_seq - список списков слов / токенов\n",
    "- label_seq - список списков тегов\n",
    "\n",
    "и возвращать из метода `__getitem__` два списка:\n",
    "- список текстовых значений (`List[str]`) из индексов токенов в сэмпле\n",
    "- список целочисленных значений (`List[int]`) из индексов соответвующих тегов\n",
    "\n",
    "P.S. В отличие от предыдущего кастомного датасет, здесь мы возвращаем два `List`'а вместо `torch.LongTensor`, так как логику формирования западдированного батча мы перенесем в `Collator` из-за специфики работы токенизатора - он сам возвращает уже западдированный тензор с индексами токенов, а для индексов тегов нам нужно будет сделать это самостоятельно по аналогии с предыдущим датасетом.\n",
    "\n",
    "**Задание. Реализуйте класс датасета TransformersDataset.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "7EoNLDOOQEMB"
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def tokenize_document(doc: Document, label2idx: Dict[str, int], tokenizer: PreTrainedTokenizer) -> TokenizedDocument:\n",
    "\n",
    "    def process_label(label: str, desired_length: int) -> List[str]:\n",
    "        if is_ent_start(label):\n",
    "            return [label] + [f'I-{ent_type(label)}'] * (desired_length - 1)\n",
    "        return [label] * desired_length\n",
    "\n",
    "    def tokenize_sentence(sentence: Iterable[str], labels: Iterable[str]) -> Tuple[LongTensor, LongTensor]:\n",
    "        all_subtokens = []\n",
    "        all_labels = []\n",
    "\n",
    "        for token, label in zip(sentence, labels):\n",
    "            subtokens = tokenizer.encode([token], add_special_tokens=False, is_split_into_words=True)\n",
    "            if not len(subtokens):  # in case tokenizer fails to split token into sub tokens\n",
    "                subtokens = [tokenizer.unk_token_id]\n",
    "            all_subtokens.extend(subtokens)\n",
    "\n",
    "            labels = map(label2idx.__getitem__, process_label(label, desired_length=len(subtokens)))\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "        return torch.tensor(all_subtokens, dtype=torch.long).long(), torch.tensor(all_labels, dtype=torch.long).long()\n",
    "\n",
    "    processed_sentences = []\n",
    "    processed_labels = []\n",
    "    fake_chars = []\n",
    "    for sentence, labels in zip(doc.sentences, doc.labels):\n",
    "        tokenized_sentence, tokenized_labels = tokenize_sentence(sentence, labels)\n",
    "        processed_sentences.append(tokenized_sentence)\n",
    "        processed_labels.append(tokenized_labels)\n",
    "        fake_chars.append(torch.tensor([[-1]] * len(tokenized_sentence), dtype=torch.long))\n",
    "\n",
    "    return TokenizedDocument(fake_chars, processed_sentences, processed_labels)  # we do not need character-level info\n",
    "\n",
    "class TransformersDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Transformers Dataset for NER.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            docs: Iterable[Document],\n",
    "            tokenizer: PreTrainedTokenizer,\n",
    "            label2idx: Dict[str, int],\n",
    "            max_sequence_length: int\n",
    "    ):\n",
    "        tokenized_documents = map(partial(tokenize_document, label2idx=label2idx, tokenizer=tokenizer), docs)\n",
    "        sentence_grouper = partial(group_sentences, max_sequence_length=max_sequence_length)\n",
    "        self._examples: List[Tuple[LongTensor, ...]] = list(chain.from_iterable(map(sentence_grouper, tokenized_documents)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._examples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[LongTensor, ...]:\n",
    "        return self._examples[idx][1:]  # skip char info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1oNc-31QEMB"
   },
   "source": [
    "Создадим три датасета:\n",
    "- *train_dataset*\n",
    "- *valid_dataset*\n",
    "- *test_dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "vqg56Jf8QEMC"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_dataset = TransformersDataset(train_docs, tokenizer=tokenizer, label2idx=label2idx, max_sequence_length=512)\n",
    "valid_dataset = TransformersDataset(val_docs, tokenizer=tokenizer, label2idx=label2idx, max_sequence_length=512)\n",
    "test_dataset = TransformersDataset(test_docs, tokenizer=tokenizer, label2idx=label2idx, max_sequence_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdIS6XrvQEMC"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IT00Pjy6QEMC",
    "outputId": "6bb42a27-08f0-49f5-96f5-52573a1a75af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([  174,  1358, 22961,   176, 14170,  1840,  1106, 21423,  9304, 10721,\n          1324,  2495, 12913,   119, 11109,  1200,  1602,  6715,  9304, 13356,\n          5999,  1820,   118,  4775,   118,  1659,  1103, 27772,  3186,  1389,\n          5500,  1163,  1113, 24438,  7719,  6194,  1122, 19786,  1114,   176,\n         14170,  5566,  1106, 11060,  1106,   188, 17315,  9304, 10721,  1324,\n          2495, 12913,  1235,  6479,  4959,  2480,  6340, 13991,  3653,  1169,\n          1129, 12086,  1106,  8892,   119,   176, 14170,  1183,   112,   188,\n          4702,  1106,  1103, 27772,  3186,  1389,  3779,   112,   188, 27431,\n          3914,  1195, 18056,   195,  7635,  4119,  1163,  1113, 26055,  3965,\n          6194, 11060,  1431,  4417,  8892,  3263,  2980,  1121,  2182,  1168,\n          1190,  9304,  5168,  1394,  1235,  1103,  3812,  5566,  1108, 27830,\n           119,   107,  1195,  1202,   183,   112,   189,  1619,  1251,  1216,\n         13710,  1272,  1195,  1202,   183,   112,   189,  1267,  1251,  4745,\n          1111,  1122,   117,   107,  1103,  5500,   112,   188,  2705, 15465,\n         11437,  2718, 17442,  3498,  4167,   185,  2225,  1500,   170,  2371,\n          4094,  1158,   119,  1119,  1163,  1748,  3812,  2025,  1108,  2320,\n          1105,  1191,  1122,  1108,  1276,  1115,  2168,  1108,  1834,  1122,\n          1431,  1129,  1678,  1118,  1103, 27772,  3186,  1389,  3779,   119,\n          1119,  1163,   170,  5835,  1314,  2370,  1118,   174,  1358,  3922,\n         12425,   175,  4047,  1584, 20497,  9022,  2879,  1106,  8214,  8892,\n         16570,   117,   188,  7136,  5026,  1105, 19245, 13408,  1116,  1121,\n          1103,  1769,  1105,  3724,  2094,  9236,  1108,   170,  3023,  2747,\n          1105,  3073,  2599, 12964,  3113,  1815,  1106,  3244,  1769,  2332,\n           119, 20497,  9022,  2879,  3000,   174,  1358,   118,  2043,  5252,\n          1170,  3756,  1121,  9304,  5168,  1394,  1105,   175, 10555,  1115,\n          1223,  8087,  2975,  8892,  1180,  2329,   171,  3292,  2042,   188,\n          5674,  2118, 20226,  4035,  2093, 20695, 13200, 23610,   113,   171,\n          2217,   114,   118,   118,  6340, 13991,  3653,   119,  1133, 20497,\n          9022,  2879,  2675,  1106,  3189,  1117,  5835,  1170,  1103,   174,\n          1358,   112,   188,  2288, 27431,  3914,   117, 22591, 12115,  3724,\n          2332,  3878,   117,  8449,  1191,  1216,  2168,  1108, 16489,  1112,\n          1175,  1108,  1178,   170,  6812,  3187,  1106,  1769,  2332,   119,\n          8492,  2944,  3922,  3907, 25338,  7490,  1742,  1260,   185,  5971,\n          8174,  1125,  2206,  4806, 20497,  9022,  2879,  1120,  1126,   174,\n          1358,  3922,  9813,   112,  2309,  1104,  3989,  8362,  9380,  2050,\n          6202,  8819,  1194,   107,  4249,  1704,  5771,   119,   107,   119,\n          1178,   175, 10555,  1105,  9304,  5168,  1394,  5534, 20497,  9022,\n          2879,   112,   188,  5835,   119,  1103,   174,  1358,   112,   188,\n          3812, 27431,  1105,  4321, 26096, 11208,  1132,  1496,  1106,  1231,\n           118, 11755,  1103,  2486,  1346,  1397,  2370,  1105,  1294, 11859,\n          1106,  1103,  2682, 27431,  3878,   119,  8892,  1138,  1263,  1151,\n          1227,  1106,  2329, 16720,  1663,   117,   170,  3575,   118, 20930,\n          3653,  1861,  1106,   171,  2217,  1134,  1110,  2475,  1106,  1138,\n          1151,  3175,  1106,  6937,  1194,  4877,  4051,  3724,  5671,   119,\n          9304, 10721,  1324,  6915,  5762,  1113, 24438,  7719,  6194,  1175,\n          1108,  1251,  5170,  1106,  1769,  2332,  1121,  1147,  8892,   117,\n          1133,  4448,  4517,  1115,   176, 14170,  1433,  5566,  1106, 11060,\n          1106,  3644,  9304, 10721,  1324,  2495, 12913,  1547,  2933, 11060,\n          1506, 27772,  3186,   119]),\n tensor([3, 7, 0, 2, 6, 0, 0, 0, 2, 6, 6, 0, 0, 0, 4, 8, 8, 8, 1, 5, 5, 0, 0, 0,\n         0, 0, 0, 3, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 2,\n         6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 5, 0, 0, 0, 0,\n         0, 3, 7, 7, 7, 0, 0, 0, 0, 4, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 1, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 4, 8, 8, 8,\n         8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 3, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 0, 0, 4,\n         8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 8, 0, 2, 6, 6, 6, 0,\n         0, 0, 0, 1, 5, 5, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 2, 6, 6, 6, 6, 6, 6, 6,\n         6, 6, 6, 6, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 8, 0, 0, 0, 0, 0, 0,\n         0, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 0, 0, 4, 8, 8, 8, 8, 8, 8, 0, 0, 0, 4, 8,\n         8, 0, 0, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 1, 5, 0, 1, 5, 5, 0, 4, 8, 8, 0, 0, 0, 0, 0, 3, 7, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 2, 6, 6, 0, 0, 0, 0, 0,\n         0, 1, 5, 0]))"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYal2icQmuD-",
    "outputId": "572bab3f-d53d-46d2-dd6f-367042966062"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 5428,   118,  5837, 18117,  5759, 15189,  1321,  1166,  1120,  1499,\n          1170,  6687,  2681,   119, 25338, 17996,  1820,   118,  4775,   118,\n          1476,  1745,  1107, 10359,  1155,   118,  1668,  1200,   185, 20473,\n         27466,  6262,  4199,  1261,  1300,  1111,  3383,  1113,   175, 22977,\n          1183,  1112,  5837, 18117,  5759, 15189,  3222,  1199, 15955,  1204,\n          1118,  1126,  6687,  1105,  3614,  2326,  1107,  1160,  1552,  1106,\n          1321,  1166,  1120,  1103,  1246,  1104,  1103,  2514,  2899,   119,\n          1147,  2215,  1113,  1499,   117,  1463,   117,  1336,  1129,  1603,\n           118,  2077,  1112,  1641,  9521, 13936, 23152,   117, 25851,  6662,\n          1105,  8910, 12210,  1155,  1804,  1107,  1113,  2681,  1229,   180,\n          3452,  1189,  1146,  1111,  1575,  1159,  1107,  1147,  4458,   118,\n          4634,  1801,  1222,  1136,  1916,  2522,  6662,   119,  1170, 11518,\n          1199, 15955,  1204,  1149,  1111,  6032,  1113,  1103,  2280,  2106,\n          1120, 11116,  1812,   117,  5837, 18117,  5759, 15189,  2925,  1147,\n          1148,  6687,  1118,  5706,  2326,  1196,  1217, 21663,  1149,  1111,\n          1853,  1545,  1114,  4035,  1403,  1931,  6187,  2881,  1105,  1183,\n         11019, 13976,  5345,  1781,  1210,  1111,  6032,   119, 13161,  1118,\n         21640,   117,  1199, 15955,  1204,  1400,   170,  4600,  1838,  1106,\n          1147,  1248,  6687,  1196, 27466,  6262,  4199,  2843,  1107,  1106,\n         15119,  1172,  1149,  1111, 21223,   119, 13936, 23152,   117,  1649,\n           117,  1440,  2218,  1106, 12699,  1147,  1499,  3205,  1170,  9468,\n         14607,   177, 13356,  8104,  1105, 11109,  1200,  1216,  1522,  1172,\n           170,  3016,  5688,  1113,  1147,  1801,  1222, 26063, 18416, 15189,\n          1120,  5312,  1926,   119,   177, 13356,  8104,   117,  1737, 17317,\n          1106,  4035,  1403,  1931,   112,   188,  1141,   118,  1285,  5420,\n           117,  4168, 18960,   117,  1117,  1148,  2899,  1432,  1104,  1103,\n          1265,   117,  1112, 13936, 23152,  1680,  3413,  1477,  1105,  1261,\n           170,  1148,  6687,  1730,  1104,  5787,   119,  1118,  1103,  1601,\n         26063, 18416, 15189,  1125,  1454,  1115,  1154,   170,  3413,   118,\n          1576,  4316,  1133,  1228,   118,  6898,  2511,  1216,  1125,   188,\n         12734, 16271,  1147,  7816,   117,  1781,  1300,  1111,  1572,  1107,\n          3615,  7318,  1105,  2128,  1172,  5205,  1113, 13606,  1111,  1421,\n          1105, 16252,  1111,  4458,   119,  1120,  1103, 13102,   117,  8910,\n         12210,  3495, 22572,  4889,  5837, 10073,  1116,   117,  1330,  1299,\n         14632,  1118,  4035,  1403,  1931,   117,  1598,  1106,  3747,  1117,\n          4217,  1112,  1119,  1723,  1117,  1300,  1111,  2532,  1113, 24438,\n          7719,  6194,  1114,  2908,  1136,  1149,  1113,   175, 22977,  1183,\n          1107,  1103,  1801,  1222,  1594,  6196,  6662,   119,  1119,  1108,\n          1218,  5534,  1118,  4035,  1403,  1931, 19953,  4551, 26087,  1150,\n          1189,  3102,  1112,  8910, 12210,  1804,  1113,  3565,  1580,  1111,\n          1978,   117,   170,  1730,  1104, 24354,   119, 25851,  6662,  2023,\n          1146,  1103,  8263,  1111,  1147,  1148,  2899,  1641,  1290,  3419,\n          1118,  7914,   192,  1766,  7723,  5759, 15189,  1106, 15118,  1111,\n          1421,  1107,  1147,  1248,  6687,   117,  1253,  1620,  2326,  1283,\n          1121, 10101,  1126,  6687,  3326,   119, 12686, 16468, 15647,  1106,\n          1306,  6601,  1183,  1261,  1565,  1111,  5787,  1133, 22572,  4889,\n          8050, 18450,   117, 13414,   117,  1105,   189,  4060,   184,   112,\n          1301,  7990,   117, 11523,   117,  1261, 25851,  6662,  1106,  3862,\n          1475,  1105,   170,  1148,  6687,  1730,  1104, 24482,   119]),\n tensor([0, 0, 3, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 2, 6, 6,\n         0, 0, 0, 0, 4, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 7, 7, 0, 3,\n         7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 0, 3, 7, 0, 3, 7, 0, 0, 0,\n         0, 0, 0, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 7, 7, 0, 0, 0,\n         3, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 3, 7, 7, 7, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 5, 0, 0, 4, 8, 8, 8, 8, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 3, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 8, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 8, 8, 8, 0, 4,\n         8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 7, 0, 1, 5, 0, 4, 8, 8, 0, 0, 0,\n         0, 1, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n         7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 7, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 7, 0, 4, 8, 8, 8,\n         8, 0, 0, 0, 0, 0, 1, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 7, 0, 0, 0, 0, 0, 0, 1,\n         5, 5, 0, 4, 8, 0, 0, 0, 0, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n         7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 7, 7, 7, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 6, 4, 8, 8, 8, 0, 0, 0,\n         0, 0, 4, 8, 8, 8, 0, 0, 0, 0, 4, 8, 8, 8, 8, 8, 0, 0, 0, 0, 3, 7, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0]))"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCXd3FWVmuKe",
    "outputId": "e35bfbaf-340f-4fba-cc39-fa84ce632cda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 5862,   118,   179, 26519,  1179,  1243,  6918,  1782,   117,  5144,\n          1161,  1107,  3774,  3326,   119,  9468,  3309,  1306, 19122,  2293,\n          2393,   118,  9562,   117, 10280,   170, 17952,  9712,  5132,  3052,\n          1820,   118,  1367,   118,  5037,   179, 26519,  1179,  1310,  1103,\n          6465,  1104,  1147,  1112,  1811,  4355,  1641,  1114,   170,  6918,\n           123,   118,   122,  1782,  1222,   188, 12577,  1465,  1107,   170,\n          1372,   172,  2899,  1801,  1113,   175, 22977,  1183,   119,  1133,\n          5144,  1161,  1486,  1147,  6920,  6941,  1172,  1107,  1103,  1248,\n          1801,  1104,  1103,  1372,   117, 13423,  1106,   170,  3774,   123,\n           118,   121,  3326,  1106, 25551,  1116,   190,  1584, 17327, 20300,\n           119,  5144,  1161,  4013,  1211,  1104,  1103,  1801,  1105,  1486,\n          1317,  9820,  4007,  1235,  1103,  5603,  1582,  2517,  1165,   190,\n          1584, 17327, 13074,   178, 18791,   188,  1324,  1377,  7170,  4854,\n          1261,  4316,  1104,   170,  1940,  1116, 15232, 10294,  1906,  5341,\n         23103,  1106, 25338,  1830,  1103,  3240,  1166,  1103, 11120,  5144,\n          6420, 13852,  1105,  1154,  1126,  3427,  5795,   119,   184, 27412,\n           188, 11220,  5437,  4786,  1189,  1612,  1104,  1103,  1782,  1107,\n          3773,  1159,   117,  6886,  1126,  8362, 26645, 20786,  1286,  2555,\n          2046,  1121,  1198,  1796,  1103,  1298,   119,  1103,  1393,  1177,\n          9756,  1204, 13911,  1108,  1773,  1107,  1126,  1112,  1811,  4355,\n          4278,  5069,  1111,  1103,  1148,  1159,   119,  2693,  2183,  1103,\n          1112,  1811,  1638,  1641,  1160,  1201,  2403,   117,   190,  1584,\n         17327, 20300,  1132,  1107,  1103,  4278,  1112, 26948,  1116,   119,\n          1160,  2513,  1121,  5341, 11122,  1107,  1103,  1314,  1565,  1904,\n          2148,   179, 26519,  1179,  1106,  1435,  1121,  1481,  1105,  7822,\n          1155,  1210,  1827,  1121,  1147,  2280,  2309,  1222,   188, 12577,\n          1465,   119, 27629,  4786,  2315, 27629,  1968,  5389,  2297,  1103,\n          2981,  1107,  1103,  5385,  1582,  2517,   117,  4703,  1106,  1246,\n           170, 20844,  5864,  3031,  2176, 11078,  1605,  5389, 12610,  2771,\n          2019,  1103,   188, 21149,  2273,  1134, 10159,  4688,  1306,  2113,\n          1813,  1691,  1106,  1138,  2262,  1133,  1173,  2148,  1106,  7324,\n          1154,  1103,  5795,   119,  1122,  1108,  1103,  1248, 18372,   171,\n         17897,  1200,  1118,   188, 12577,  1465,  1107,  1300,  1904,   119,\n          8919,  1144,  9995,   170, 27261,  1116,  3152,  1106, 22205,   170,\n          1263,  3240,  1154,  1103,  1298,  1107,  1103,  5731,  1582,  2517,\n          1133,  1178,  2374,  1106, 23448,  1204,  1122,  1154,  1103,  1499,\n          2655,  1104,  2113,  1813,   112,   188,  2273,   119,  9468,  2692,\n           179,  5926,  9574,  1813,  1125,  1549,   188, 12577,  1465,  1103,\n          1730,  1114,   170,  1218,   118,  4168, 23103,  1107,  1103,  5001,\n          2517,   119,   179, 26519,  1179,  1173,  3390,  8098,  1106,  1103,\n           188, 21149,  6180,  1298,  1111,  1211,  1104,  1103,  1342,  1133,\n          6034, 13275,  1174,  1103,   188, 21149,  6465,   119,  2113,  1813,\n          1865,  1228,  2503, 14419,  7747,  1152,  1225,   119,   179, 26519,\n          1179,  2154,   188,  6583, 24181,  3702,  1163,   131,   112,   112,\n          1103,   188, 21149,  1319,  2273,  4132,  6918,  1111,  1366,   119,\n          1103,   188, 21149,  1116,  2297,  1346,  1105,  1173,  1307,  5341,\n          1193,  1105,  3399,  1263,  7318,  1134,  1189,  1122,  1662,  1111,\n          1366,   119,   112,   112,   179, 26519,  1179,   117,  1884,   118,\n          5654,  1104,  1103,  1362,  4355,  1107,  1617,  1105,  3616,  3116,\n          1107,  1103,  1362,  1118, 20497,  8057,   117,  1132,  9122,  1116,\n          1106, 12699,  1147,  1641,  1303,   119]),\n tensor([0, 0, 1, 5, 5, 0, 0, 0, 0, 4, 8, 0, 0, 0, 0, 4, 8, 8, 8, 8, 1, 5, 5, 0,\n         1, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 1, 5, 5, 0, 0, 0, 0, 0, 2, 6, 6, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 1, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         1, 5, 5, 5, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n         6, 6, 0, 4, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 4, 8, 8, 8, 8, 8, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 6,\n         0, 0, 0, 0, 0, 2, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 6, 0, 0, 0,\n         0, 0, 1, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 1, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 5, 0, 4, 8,\n         8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 8, 8, 8, 8, 8,\n         8, 0, 0, 0, 2, 6, 0, 0, 0, 4, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 5, 0, 0, 0, 0, 0, 4, 8, 8, 8, 8,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 4, 8, 0, 0, 0, 0, 4, 8, 8, 8, 8, 8, 0, 0, 1, 5, 5, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 5, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 2, 6, 0, 0, 4, 8, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 5, 0,\n         4, 8, 8, 8, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 6, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 5, 0, 0, 0,\n         0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0]))"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4R605vAnYT9",
    "outputId": "1b6542ed-4520-4093-dddf-f9404e059917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reject asserts imbrace monke\n"
     ]
    }
   ],
   "source": [
    "# assert len(train_dataset) == 14986, \"Неправильная длина train_dataset\"\n",
    "# assert len(valid_dataset) == 3465, \"Неправильная длина valid_dataset\"\n",
    "# assert len(test_dataset) == 3683, \"Неправильная длина test_dataset\"\n",
    "#\n",
    "# assert train_dataset[0][0] == ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'], \"Неправильно сформированный train_dataset\"\n",
    "# assert train_dataset[0][1] == [3,0,2,0,0,0,2,0,0], \"Неправильно сформированный train_dataset\"\n",
    "#\n",
    "# assert valid_dataset[0][0] == ['cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top', 'after', 'innings', 'victory', '.'], \"Неправильно сформированный valid_dataset\"\n",
    "# assert valid_dataset[0][1] == [0,0,3,0,0,0,0,0,0,0,0], \"Неправильно сформированный valid_dataset\"\n",
    "#\n",
    "# assert test_dataset[0][0] == ['soccer', '-', 'japan', 'get', 'lucky', 'win', ',', 'china', 'in', 'surprise', 'defeat', '.'], \"Неправильно сформированный test_dataset\"\n",
    "# assert test_dataset[0][1] == [0,0,1,0,0,0,0,4,0,0,0,0], \"Неправильно сформированный test_dataset\"\n",
    "#\n",
    "# print(\"Тесты пройдены!\")\n",
    "\n",
    "print('Reject asserts imbrace monke')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zP_6iQnQEMC"
   },
   "source": [
    "Реализуем новый `Collator`.\n",
    "\n",
    "Инициализировать коллатор будет 3 аргументами:\n",
    "- токенизатор\n",
    "- параметры токенизатора в виде словаря (затем используем как `**kwargs`)\n",
    "- id спецтокена для последовательностей тегов (значение -1)\n",
    "\n",
    "Метод `__call__` на вход принимает батч, а именно список кортежей того, что нам возвращается из датасета. В нашем случае это список кортежей двух int64 тензоров - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
    "\n",
    "На выходе мы хотим получить два тензора:\n",
    "- западденные индексы слов / токенов\n",
    "- западденные индексы тегов\n",
    "\n",
    "**Задание. Реализуйте класс коллатора TransformersCollator.** **<font color='red'>(2 балла)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "BonAp65jQEMD"
   },
   "outputs": [],
   "source": [
    "class TransformersCollator:\n",
    "    \"\"\"\n",
    "    Collator that handles variable-size sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_padding_value: int, label_padding_value: int):\n",
    "        self.token_padding_value = token_padding_value\n",
    "        self.label_padding_value = label_padding_value\n",
    "\n",
    "    def __call__(self, batch: List[Tuple[LongTensor, LongTensor]]) -> Dict[str, LongTensor]:\n",
    "        tokens, labels = zip(*batch)\n",
    "        masks = list(map(partial(torch.ones_like, dtype=torch.bool), tokens))\n",
    "        return {\n",
    "            \"input_ids\": pad_sequence(tokens, batch_first=True, padding_value=self.token_padding_value).long(),\n",
    "            \"labels\": pad_sequence(labels, batch_first=True, padding_value=self.label_padding_value).long(),\n",
    "            \"attention_mask\": pad_sequence(masks, batch_first=True, padding_value=False).long()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "5sCDaxR6QEMD"
   },
   "outputs": [],
   "source": [
    "collator = TransformersCollator(token_padding_value=tokenizer.pad_token_id, label_padding_value=-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eirev0N_QEMD"
   },
   "source": [
    "Теперь всё готово, чтобы задать `DataLoader`'ы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "9JDrLC6pQEME"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # для корректных замеров метрик оставить batch_size=1\n",
    "    shuffle=False, # для корректных замеров метрик оставить shuffle=False\n",
    "    collate_fn=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3zGjEDHQEME"
   },
   "source": [
    "Посмотрим на то, что мы получили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "KSWcYEAWQEME"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "tokens = batch['input_ids']\n",
    "labels = batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NTcdU1BlQEME",
    "outputId": "94ac83e6-a90e-4247-a8c9-46fb487d6d91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 5428,   118,  4035,  1403,  1931,   191,   185,  8552, 13946,  1509,\n          2774,  2794,  4015,   119, 25338, 17996,  1820,   118,  4775,   118,\n          1572,  2794,  4015,  1113,  1103,  1503,  1285,  1104,  1103,  1503,\n          1105,  1509,  2774,  1206,  4035,  1403,  1931,  1105,   185,  8552,\n         13946,  1120,  1103, 13102,  1113,  2068,  2149,  6194,   131,  4035,\n          1403,  1931,  1148,  6687,  2724,  1545,   113,   179,   119, 14869,\n          2254,  9920,   117,   176,   119, 24438,  1766,  3186,  4335,   132,\n         20049, 24750,  1197,  1128,  7221,   125,   118,  4573,   114,   185,\n          8552, 13946,  1148,  6687,   113, 12292, 25325,   118,   122,   114,\n         21718, 11394,  1126,  7200,   172,   172, 27421,   171,  1884,  4661,\n         20039,   170, 11787,  1197,  1177, 10390,  1233,   172,  1884,  4661,\n           171,   172, 27421,  3993,   178,  3174,  1584, 18257,  4611,   172,\n         26036,  9349,   171,   182, 11781,  2716,  5391,  1107, 20123,  2312,\n           118, 23449,   118,  5871,  4426,   172,   177, 13356,  8104,   171,\n           182, 11781,  2716,  2588, 21718, 24891, 12477, 19921,  1136,  1149,\n           123,  1112,  8914,   182,  1358,  3361,  1777,  2822,  1136,  1149,\n           122,  3908,  1116,   113,   171,   118,   125,  5682,   118,   124,\n           183,  1830,   118,  1429,   114,  1407,  1703,   113,  1111,  1300,\n         10267,   114,  3081,  1580,  2303,  1104, 10267,   131,   122,   118,\n          9920,   123,   118, 26479,   124,   118,  3081,  1527,   125,   118,\n          3081,  1527,  1106,  7693,   131,  1108,  4060,   170, 27311,  1306,\n           117,   182,  8136,  1179,   180,  3822,   117,   182, 13148,  1777,\n          4426, 18257,  4611,   117, 20049, 24750,  1197,  1128,  7221,   117,\n           182, 10559,  2312, 22678, 10908,  1306, 11518,   113,  1106,  2236,\n           114,   131,  5837, 10073,  1116,  1367,   118,   122,   118,  5465,\n           118,   121,   117,   182, 11781,  2716,  1659,   118,   127,   118,\n          4376,   118,   123,   117,   172, 27421,  1853,   118,   127,   118,\n          3324,   118,   122,   117,  1884,  4661,  1489,   119,   124,   118,\n           125,   118,  2532,   118,   122,   117, 21718,  6137,  4109,  1542,\n           118,   121,   118,  5539,   118,   121],\n        [ 9391,  2856,  1106, 12418,  5303,  1233,   172,  1964,  2956,  4582,\n           118,  1228,   119, 12418,  5053,  4567,  1820,   118,  4775,   118,\n          1743,  1103, 12418,  5303, 15647, 22719, 26055,  3965,  6194,  2675,\n          1106,  1131, 23534,   170,  4550, 11723,  1103,   185, 26815,  1104,\n          5463, 27482,   191,  7531,  1202,   187,  2660,  1202,  2093,  1106,\n         10974,  5684,   117,  3878,  1163,   119,  3878,  1163,  1103, 22719,\n          2992,  2856,  1155,  3685,  7663, 20928,  1107,  1103,  1236,  1104,\n           172,  1964,  2956,   112,   188,  4582,   118,  1228,   119,  1103,\n          4018,  1108,  1508,  1977,  1118, 14516,  1179,   119,   179,  6787,\n          5048, 21320,  1186,  3840,  4487,   117,  1150,  1125,  3795,  1146,\n          1103,  4550,   119,  1103, 22719,  2992,  1145,  1126, 14787, 11572,\n           170,  7359,  1683,  1104,  3840,  4487,   112,   188,  4550,  1134,\n          1125,  4110,  1106,  1260, 12892,  7143,  1121,   191,  7531,   112,\n           188,   185, 26815,  1106,  2918,  6557,  3203,   119,   118,   118,\n          1209, 18331,   188,  8401, 23701,   117, 12418,  5053,  4567,  2371,\n          6077,  3731,   118,  5391,   118, 22803,  1568, 19297,  1604,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]], device='cuda:0')"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7ZTh97-QEME",
    "outputId": "7eb951ed-2bf6-475a-f799-07551fc3b5c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[   0,    0,    1,    5,    5,    0,    1,    5,    5,    0,    0,    0,\n            0,    0,    1,    5,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    5,\n            5,    0,    1,    5,    5,    0,    1,    5,    0,    0,    0,    0,\n            0,    1,    5,    5,    0,    0,    0,    0,    0,    4,    8,    8,\n            8,    0,    0,    4,    8,    8,    8,    8,    0,    0,    4,    8,\n            8,    4,    8,    0,    0,    0,    0,    1,    5,    5,    0,    0,\n            0,    0,    0,    0,    0,    0,    4,    8,    8,    8,    0,    4,\n            8,    0,    4,    8,    0,    4,    8,    8,    8,    8,    8,    0,\n            4,    8,    0,    4,    8,    0,    4,    8,    8,    8,    8,    0,\n            4,    8,    0,    4,    8,    8,    0,    4,    8,    8,    8,    8,\n            8,    8,    8,    0,    4,    8,    8,    0,    4,    8,    8,    0,\n            4,    8,    8,    8,    0,    0,    0,    4,    8,    8,    8,    8,\n            8,    8,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    4,    8,    8,    8,    8,    0,    4,    8,    8,    8,    8,\n            0,    4,    8,    8,    8,    8,    8,    0,    4,    8,    8,    8,\n            8,    0,    4,    8,    8,    8,    8,    8,    0,    0,    0,    0,\n            0,    0,    4,    8,    8,    0,    0,    0,    0,    0,    0,    0,\n            0,    4,    8,    8,    0,    0,    0,    0,    0,    0,    0,    0,\n            4,    8,    0,    0,    0,    0,    0,    0,    0,    0,    4,    8,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    4,    8,\n            8,    0,    0,    0,    0,    0,    0,    0],\n        [   0,    0,    0,    1,    5,    5,    3,    7,    7,    0,    0,    0,\n            0,    1,    5,    5,    0,    0,    0,    0,    0,    0,    2,    6,\n            6,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    3,    7,    7,    7,    7,    7,\n            7,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    3,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    3,    7,\n            7,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    1,    5,    0,    4,    8,    8,    8,    8,    8,    8,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    3,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    4,    8,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    3,    7,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    4,    8,    8,    8,\n            8,    0,    1,    5,    5,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n         -100, -100, -100, -100, -100, -100, -100, -100]], device='cuda:0')"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMprtk9bodM9",
    "outputId": "d93f5b97-fbad-4538-f37f-068c7bbac104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reject asserts imbrace monke\n"
     ]
    }
   ],
   "source": [
    "# train_tokens, train_labels = next(iter(\n",
    "#     torch.utils.data.DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=2,\n",
    "#         shuffle=False,\n",
    "#         collate_fn=collator,\n",
    "#     )\n",
    "# ))\n",
    "# assert torch.equal(train_tokens['input_ids'], torch.tensor([[  101,   174,  1358, 22961,   176, 14170,  1840,  1106, 21423,  9304, 10721,  1324,  2495, 12913,   119,   102], [  101, 11109,  1200,  1602,  6715,   102,     0,     0,     0,     0,    0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
    "# assert torch.equal(train_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
    "# assert torch.equal(train_labels, torch.tensor([[-1,  3, -1,  0,  2, -1,  0,  0,  0,  2, -1, -1,  0, -1,  0, -1], [-1,  4, -1,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "#\n",
    "# valid_tokens, valid_labels = next(iter(\n",
    "#     torch.utils.data.DataLoader(\n",
    "#         valid_dataset,\n",
    "#         batch_size=2,\n",
    "#         shuffle=False,\n",
    "#         collate_fn=collator,\n",
    "#     )\n",
    "# ))\n",
    "# assert torch.equal(valid_tokens['input_ids'], torch.tensor([[  101,  5428,   118,  5837, 18117,  5759, 15189,  1321,  1166,  1120,  1499,  1170,  6687,  2681,   119,   102], [  101, 25338, 17996,  1820,   118,  4775,   118,  1476,   102,     0,     0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
    "# assert torch.equal(valid_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
    "# assert torch.equal(valid_labels, torch.tensor([[-1,  0,  0,  3, -1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0, -1], [-1,  1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "#\n",
    "# test_tokens, test_labels = next(iter(\n",
    "#     torch.utils.data.DataLoader(\n",
    "#         test_dataset,\n",
    "#         batch_size=2,\n",
    "#         shuffle=False,\n",
    "#         collate_fn=collator,\n",
    "#     )\n",
    "# ))\n",
    "# assert torch.equal(test_tokens['input_ids'], torch.tensor([[  101,  5862,   118,   179, 26519,  1179,  1243,  6918,  1782,   117,  5144,  1161,  1107,  3774,  3326,   119,   102], [  101,  9468,  3309,  1306, 19122,  2293,   102,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])), \"Похоже на ошибку в коллаторе\"\n",
    "# assert torch.equal(test_tokens['attention_mask'], torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])), \"Похоже на ошибку в коллаторе\"\n",
    "# assert torch.equal(test_labels, torch.tensor([[-1,  0,  0,  1, -1, -1,  0,  0,  0,  0,  4, -1,  0,  0,  0,  0, -1], [-1,  4, -1, -1,  8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])), \"Похоже на ошибку в коллаторе\"\n",
    "#\n",
    "# print(\"Тесты пройдены!\")\n",
    "print('Reject asserts imbrace monke')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m-taH0SQEMF"
   },
   "source": [
    "В библиотеке **transformers** есть классы для модели BERT, уже настроенные под решение конкретных задач, с соответствующими головами классификации. Для задачи NER будем использовать класс `BertForTokenClassification`.\n",
    "\n",
    "По аналогии с токенизаторами, мы можем использовать класс `AutoModelForTokenClassification`, который по названию модели сам определит, какой класс нужен для инициализации модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "x6tq_i7JQEMF"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Vma9yj0zQEMF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label2idx)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "Imv-6gAQQEMG"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "LAdHfn4oQEMG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "TokenClassifierOutput(loss=tensor(2.4413, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-0.4908,  0.0150,  0.1640,  ...,  0.1318, -0.4291, -0.0025],\n         [-0.4051, -0.2246,  0.1744,  ...,  0.0928, -0.4277, -0.1350],\n         [-0.6981, -0.4619,  0.2565,  ...,  0.0635, -0.1540, -0.4349],\n         ...,\n         [-0.3789,  0.1075,  0.2735,  ..., -0.2194, -0.4260, -0.2696],\n         [-0.3482,  0.0192, -0.0790,  ..., -0.3009, -0.4384, -0.1082],\n         [-0.3326,  0.1791,  0.2249,  ..., -0.2535, -0.4911, -0.2031]],\n\n        [[-0.3696,  0.1572,  0.1812,  ..., -0.0673, -0.3323, -0.1606],\n         [-0.4575,  0.0034, -0.0377,  ..., -0.1399, -0.3985, -0.3150],\n         [-0.3796, -0.2140,  0.1281,  ..., -0.4739, -0.5786, -0.3926],\n         ...,\n         [-0.5417,  0.0061,  0.1495,  ..., -0.0891, -0.2574, -0.0135],\n         [-0.4983,  0.0869,  0.1972,  ..., -0.0757, -0.2450,  0.0243],\n         [-0.4796,  0.0358,  0.2536,  ..., -0.0928, -0.2840, -0.0416]]],\n       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**batch)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-kTke_8QEMG",
    "outputId": "d3ccee80-a0c4-429a-8d62-787faf8543d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тесты пройдены!\n"
     ]
    }
   ],
   "source": [
    "assert 2 < outputs.loss < 3\n",
    "\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "5Ana4qGKeHrN"
   },
   "outputs": [],
   "source": [
    "# создадим SummaryWriter для эксперимента с BiLSTMModel\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"logs/Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "class TransformersWrapper(Module):\n",
    "    \"\"\"\n",
    "    Wrapper over Huggingface models to satisfy our training loop interface.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: PreTrainedModel):\n",
    "        super().__init__()\n",
    "        self._model = model\n",
    "\n",
    "\n",
    "    def forward(self, input_ids: LongTensor, attention_mask: BoolTensor, labels: Optional[LongTensor] = None, return_predictions: bool = False) -> Union[Tensor, Tuple[Tensor, LongTensor]]:\n",
    "        output = self._model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = output.loss\n",
    "            if return_predictions:\n",
    "                return loss, torch.argmax(output.logits, dim=-1).long()\n",
    "            return loss\n",
    "\n",
    "        return torch.argmax(output.logits, dim=-1).long()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sNuFPRdQEMH"
   },
   "source": [
    "### Эксперименты\n",
    "\n",
    "Проведите эксперименты на данных. Настраивайте параметры по валидационной выборке, не используя тестовую. Ваше цель — настроить сеть так, чтобы качество модели по F1-macro мере на валидационной и тестовой выборках было не меньше 0.9. \n",
    "\n",
    "Сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IfkN20lrN0J"
   },
   "source": [
    "Вы можете использовать ту же самую функцию train, что и до этого за тем исключением, что вместо инференса `model(tokens)` нужно делать `model(**tokens)`, а вместо `outputs` использовать `outputs[\"logits\"].transpose(1, 2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iyZUFddzYE5"
   },
   "source": [
    "**Задание. Проведите эксперименты.** **<font color='red'>(2 балла)</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "label_mapping = {v: k for k, v in label2idx.items()}\n",
    "label_mapping = np.array([label_mapping[idx] for idx in range(len(label_mapping))])\n",
    "\n",
    "exp_id = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "Iv92wK5P7pjl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 946 documents and 14041 sentences from conll03/data/train.txt\n",
      "Read 216 documents and 3250 sentences from conll03/data/valid.txt\n",
      "Read 231 documents and 3453 sentences from conll03/data/test.txt\n"
     ]
    }
   ],
   "source": [
    "transformer_model = 'roberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model, add_prefix_space=True)\n",
    "transformer = AutoModelForTokenClassification.from_pretrained(transformer_model, num_labels=len(label_mapping))\n",
    "model = TransformersWrapper(transformer).to(DEVICE)\n",
    "\n",
    "convert_to_iob = True\n",
    "train_docs = read_conll2003(Path('conll03/data/train.txt'), convert_to_iob=convert_to_iob, lower=False)\n",
    "val_docs = read_conll2003(Path('conll03/data/valid.txt'), convert_to_iob=convert_to_iob, lower=False)\n",
    "test_docs = read_conll2003(Path('conll03/data/test.txt'), convert_to_iob=convert_to_iob, lower=False)\n",
    "\n",
    "collator = TransformersCollator(token_padding_value=tokenizer.pad_token_id, label_padding_value=-100)\n",
    "\n",
    "max_sequence_length = 512\n",
    "\n",
    "train_dataset = TransformersDataset(train_docs, tokenizer=tokenizer, label2idx=label2idx, max_sequence_length=max_sequence_length)\n",
    "valid_dataset = TransformersDataset(val_docs, tokenizer=tokenizer, label2idx=label2idx, max_sequence_length=max_sequence_length)\n",
    "test_dataset = TransformersDataset(test_docs, tokenizer=tokenizer, label2idx=label2idx, max_sequence_length=max_sequence_length)\n",
    "\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 32\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collator)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=eval_batch_size, shuffle=False, collate_fn=collator)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=eval_batch_size,shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch, best metric: nan\n",
      "2 epoch, best metric: 0.826\n",
      "3 epoch, best metric: 0.899\n",
      "4 epoch, best metric: 0.926\n",
      "5 epoch, best metric: 0.937\n",
      "6 epoch, best metric: 0.941\n",
      "7 epoch, best metric: 0.946\n",
      "8 epoch, best metric: 0.954\n",
      "9 epoch, best metric: 0.955\n",
      "10 epoch, best metric: 0.955\n",
      "11 epoch, best metric: 0.956\n",
      "12 epoch, best metric: 0.959\n",
      "13 epoch, best metric: 0.960\n",
      "14 epoch, best metric: 0.960\n",
      "15 epoch, best metric: 0.960\n",
      "16 epoch, best metric: 0.960\n",
      "17 epoch, best metric: 0.960\n",
      "18 epoch, best metric: 0.960\n"
     ]
    }
   ],
   "source": [
    "exp_id += 1\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"logs/Transformer_{exp_id}\")\n",
    "\n",
    "lr = 2e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = train_loop(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=valid_dataloader,\n",
    "    label_mapping=label_mapping,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=train_batch_size,\n",
    "    initial_lr=lr,\n",
    "    drop_factor_lr=0.5,\n",
    "    min_lr=1e-6,\n",
    "    reload_patience=10,\n",
    "    lr_schedule='adaptive',\n",
    "    max_epochs=2000,\n",
    "    clip_grads=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_epochs=0.1,\n",
    "    val_period=100,\n",
    "    disable_amp=False,\n",
    "    target_metric='f1_macro',\n",
    "    ignore_index=-100,\n",
    "    writer=writer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_LOC': 0.9811423886307734,\n",
      " 'f1_MISC': 0.908992999461497,\n",
      " 'f1_ORG': 0.9631833395314242,\n",
      " 'f1_PER': 0.988036976617727,\n",
      " 'f1_macro': 0.9603389260603554,\n",
      " 'f1_micro': 0.9679373895480939,\n",
      " 'f1_weighted': 0.9680619861716139,\n",
      " 'loss': 0.03214042199154695,\n",
      " 'precision_LOC': 0.9814106068890104,\n",
      " 'precision_MISC': 0.8950159066808059,\n",
      " 'precision_ORG': 0.9606824925816023,\n",
      " 'precision_PER': 0.9842903575297941,\n",
      " 'precision_macro': 0.9553498409203032,\n",
      " 'precision_micro': 0.9639624539054643,\n",
      " 'precision_weighted': 0.9642591020550375,\n",
      " 'recall_LOC': 0.9808743169398907,\n",
      " 'recall_MISC': 0.9234135667396062,\n",
      " 'recall_ORG': 0.9656972408650261,\n",
      " 'recall_PER': 0.9918122270742358,\n",
      " 'recall_macro': 0.9654493379046897,\n",
      " 'recall_micro': 0.9719452425215481,\n",
      " 'recall_weighted': 0.971945242521548}\n"
     ]
    }
   ],
   "source": [
    "metrics = validate(model, valid_dataloader, label_mapping, -100)\n",
    "pprint(metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_LOC': 0.9364889155182745,\n",
      " 'f1_MISC': 0.801959412176347,\n",
      " 'f1_ORG': 0.9150209455415917,\n",
      " 'f1_PER': 0.9840575179743669,\n",
      " 'f1_macro': 0.9093816978026451,\n",
      " 'f1_micro': 0.9266006367173683,\n",
      " 'f1_weighted': 0.9269700360204776,\n",
      " 'loss': 0.14623985532671213,\n",
      " 'precision_LOC': 0.9348086124401914,\n",
      " 'precision_MISC': 0.7870879120879121,\n",
      " 'precision_ORG': 0.9020648967551622,\n",
      " 'precision_PER': 0.9855979962429555,\n",
      " 'precision_macro': 0.9023898543815553,\n",
      " 'precision_micro': 0.9204146170063247,\n",
      " 'precision_weighted': 0.9212551014309033,\n",
      " 'recall_LOC': 0.9381752701080432,\n",
      " 'recall_MISC': 0.8174037089871612,\n",
      " 'recall_ORG': 0.928354584092289,\n",
      " 'recall_PER': 0.982521847690387,\n",
      " 'recall_macro': 0.91661385271947,\n",
      " 'recall_micro': 0.9328703703703703,\n",
      " 'recall_weighted': 0.9328703703703705}\n"
     ]
    }
   ],
   "source": [
    "metrics = validate(model, test_dataloader, label_mapping, -100)\n",
    "pprint(metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XlI3cb1QEL2"
   },
   "source": [
    "## Часть 4 - Бонус. BiLSTMAttention-теггер (2 баллa)\n",
    "\n",
    "Необходимо провести те же самые эксперименты как и в части 2, но уже с использованием усовершенствованной архитектуры теггера BiLSTM с Attention механизмом.\n",
    "\n",
    "**Обратите внимание**, что реализовывать Attention самому не нужно, можно использовать `torch.nn.MultiheadAttention`.\n",
    "\n",
    "Также сделайте выводы о качестве модели, переобучении, чувствительности архитектуры к выбору гиперпараметров и проведите небольшой сравнительный анализ с предыдущей архитектурой. Оформите результаты экспериментов в виде мини-отчета (в этом же ipython notebook).\n",
    "\n",
    "**Задание. Реализуйте класс модели BiLSTMAttn.** **<font color='red'>(1 балл)</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "_MyLQp047yID"
   },
   "outputs": [],
   "source": [
    "BiLSTMAttn = partial(BiLSTM, head_type='attention', attention_params=AttentionParameters(\n",
    "    dropout=0.1,\n",
    "    dims=256,\n",
    "    num_heads=8\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezh9kLTkQEL9"
   },
   "source": [
    "**Задание. Проведите эксперименты и побейте метрику из части 2.** **<font color='red'>(1 балл)</font>**\n",
    "\n",
    "P.S. Eсли качества увеличить не получилось, это нужно обосновать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "sE1C1tzEQEL-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 946 documents and 14041 sentences from conll03/data/train.txt\n",
      "Read 216 documents and 3250 sentences from conll03/data/valid.txt\n",
      "Read 231 documents and 3453 sentences from conll03/data/test.txt\n",
      "Building pretrained vocabulary from glove/data/glove.6B.300d.txt\n",
      "Vocabulary size: 400000\n",
      "Final vocabulary size: 400023\n",
      "Read 946 documents and 14041 sentences from conll03/data/train.txt\n",
      "Read 216 documents and 3250 sentences from conll03/data/valid.txt\n",
      "Read 231 documents and 3453 sentences from conll03/data/test.txt\n",
      "Building pretrained vocabulary from glove/data/glove.6B.300d.txt\n",
      "Vocabulary size: 400000\n",
      "Final vocabulary size: 400023\n"
     ]
    }
   ],
   "source": [
    "embeddings_path = Path('glove/data/glove.6B.300d.txt')\n",
    "\n",
    "convert_to_iob = True\n",
    "train_docs = read_conll2003(Path('conll03/data/train.txt'), convert_to_iob=convert_to_iob, lower=False)\n",
    "val_docs = read_conll2003(Path('conll03/data/valid.txt'), convert_to_iob=convert_to_iob, lower=False)\n",
    "test_docs = read_conll2003(Path('conll03/data/test.txt'), convert_to_iob=convert_to_iob, lower=False)\n",
    "\n",
    "lower = True\n",
    "char2idx = get_char2idx(token2cnt.keys(), num_embedding=False)\n",
    "token2cnt = get_token_counts(train_docs, lower=lower)\n",
    "token2idx = get_token2idx(token2cnt, min_count=10, base_dict=glove2vocab(embeddings_path))\n",
    "\n",
    "collator = NERCollator(\n",
    "    char_padding_value=c2idx['<PAD>'],\n",
    "    token_padding_value=t2idx['<PAD>'],\n",
    "    label_padding_value=-1,\n",
    ")\n",
    "\n",
    "max_token_length = 20\n",
    "max_sequence_length = 128\n",
    "\n",
    "train_dataset = NERDataset(\n",
    "    train_docs,\n",
    "    char2idx=char2idx, token2idx=token2idx, label2idx=label2idx,\n",
    "    max_token_length=max_token_length, max_sequence_length=max_sequence_length,\n",
    "    lower=lower\n",
    ")\n",
    "valid_dataset = NERDataset(\n",
    "    val_docs,\n",
    "    char2idx=char2idx, token2idx=token2idx, label2idx=label2idx,\n",
    "    max_token_length=max_token_length, max_sequence_length=max_sequence_length,\n",
    "    lower=lower\n",
    ")\n",
    "test_dataset = NERDataset(\n",
    "    test_docs,\n",
    "    char2idx=char2idx, token2idx=token2idx, label2idx=label2idx,\n",
    "    max_token_length=max_token_length, max_sequence_length=max_sequence_length,\n",
    "    lower=lower\n",
    ")\n",
    "\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 512\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collator)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=eval_batch_size, shuffle=False, collate_fn=collator)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=eval_batch_size,shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting word embeddings from glove/data/glove.6B.300d.txt\n",
      "Extracted 400000 embeddings\n",
      "1 epoch, best metric: nan\n",
      "2 epoch, best metric: nan\n",
      "3 epoch, best metric: nan\n",
      "4 epoch, best metric: 0.084\n",
      "5 epoch, best metric: 0.373\n",
      "6 epoch, best metric: 0.647\n",
      "7 epoch, best metric: 0.759\n",
      "8 epoch, best metric: 0.810\n",
      "9 epoch, best metric: 0.838\n",
      "10 epoch, best metric: 0.843\n",
      "11 epoch, best metric: 0.851\n",
      "12 epoch, best metric: 0.862\n",
      "13 epoch, best metric: 0.881\n",
      "14 epoch, best metric: 0.881\n",
      "15 epoch, best metric: 0.881\n",
      "16 epoch, best metric: 0.885\n",
      "17 epoch, best metric: 0.893\n",
      "18 epoch, best metric: 0.893\n",
      "19 epoch, best metric: 0.899\n",
      "20 epoch, best metric: 0.900\n",
      "21 epoch, best metric: 0.900\n",
      "22 epoch, best metric: 0.900\n",
      "23 epoch, best metric: 0.905\n",
      "24 epoch, best metric: 0.905\n",
      "25 epoch, best metric: 0.905\n",
      "26 epoch, best metric: 0.911\n",
      "27 epoch, best metric: 0.911\n",
      "28 epoch, best metric: 0.912\n",
      "29 epoch, best metric: 0.912\n",
      "30 epoch, best metric: 0.912\n",
      "31 epoch, best metric: 0.912\n",
      "32 epoch, best metric: 0.912\n",
      "33 epoch, best metric: 0.912\n",
      "34 epoch, best metric: 0.912\n",
      "35 epoch, best metric: 0.912\n",
      "36 epoch, best metric: 0.912\n",
      "37 epoch, best metric: 0.912\n",
      "38 epoch, best metric: 0.912\n",
      "39 epoch, best metric: 0.912\n",
      "40 epoch, best metric: 0.912\n",
      "41 epoch, best metric: 0.912\n",
      "42 epoch, best metric: 0.912\n",
      "43 epoch, best metric: 0.912\n",
      "44 epoch, best metric: 0.912\n",
      "45 epoch, best metric: 0.912\n"
     ]
    }
   ],
   "source": [
    "exp_id += 1\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"logs/BiLSTMAttnModel_{exp_id}\")\n",
    "\n",
    "lr = 0.015\n",
    "model = BiLSTMAttn(\n",
    "    num_chars=len(char2idx),\n",
    "    vocab=token2idx,\n",
    "    char_embedding_dim=50,\n",
    "    token_embedding_dim=300,\n",
    "    pretrained_embeddings=embeddings_path,\n",
    "    freeze_pretrained_embeddings=True,\n",
    "    hidden_size=300,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    "    interlayer_dropout=0.0,\n",
    "    bidirectional=True,\n",
    "    n_classes=len(label2idx),\n",
    "    max_token_length=max_token_length,\n",
    "    char_padding_idx=char2idx['<PAD>'],\n",
    "    token_padding_idx=token2idx['<PAD>'],\n",
    "    criterion_type='crf',\n",
    "    train_initial_state=True\n",
    ").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = train_loop(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=valid_dataloader,\n",
    "    label_mapping=label_mapping,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=train_batch_size,\n",
    "    initial_lr=lr,\n",
    "    drop_factor_lr=0.5,\n",
    "    min_lr=1e-4,\n",
    "    reload_patience=10,\n",
    "    lr_schedule='adaptive',\n",
    "    max_epochs=2000,\n",
    "    clip_grads=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_epochs=2.0,\n",
    "    val_period=500,\n",
    "    disable_amp=False,\n",
    "    target_metric='f1_macro',\n",
    "    ignore_index=-1,\n",
    "    writer=writer\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_LOC': 0.9496442255062945,\n",
      " 'f1_MISC': 0.6780422092501123,\n",
      " 'f1_ORG': 0.8657047724750276,\n",
      " 'f1_PER': 0.8288679700699976,\n",
      " 'f1_macro': 0.8305647943253579,\n",
      " 'f1_micro': 0.8449752494696314,\n",
      " 'f1_weighted': 0.8496427630623489,\n",
      " 'loss': 0.05635428614914417,\n",
      " 'precision_LOC': 0.9512061403508771,\n",
      " 'precision_MISC': 0.575019040365575,\n",
      " 'precision_ORG': 0.8590308370044053,\n",
      " 'precision_PER': 0.9296155928532756,\n",
      " 'precision_macro': 0.8287179026435333,\n",
      " 'precision_micro': 0.8473053892215568,\n",
      " 'precision_weighted': 0.8701821647626184,\n",
      " 'recall_LOC': 0.9480874316939891,\n",
      " 'recall_MISC': 0.8260393873085339,\n",
      " 'recall_ORG': 0.87248322147651,\n",
      " 'recall_PER': 0.747822299651568,\n",
      " 'recall_macro': 0.8486080850326503,\n",
      " 'recall_micro': 0.8426578906127566,\n",
      " 'recall_weighted': 0.8426578906127566}\n"
     ]
    }
   ],
   "source": [
    "metrics = validate(model, valid_dataloader, label_mapping, -100)\n",
    "pprint(metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_LOC': 0.8944693572496264,\n",
      " 'f1_MISC': 0.5717488789237668,\n",
      " 'f1_ORG': 0.8158131176999102,\n",
      " 'f1_PER': 0.8120342257797405,\n",
      " 'f1_macro': 0.773516394913261,\n",
      " 'f1_micro': 0.8004300719543462,\n",
      " 'f1_weighted': 0.8079280590649875,\n",
      " 'loss': 0.09404996037483215,\n",
      " 'precision_LOC': 0.8910065515187612,\n",
      " 'precision_MISC': 0.4709141274238227,\n",
      " 'precision_ORG': 0.8049645390070922,\n",
      " 'precision_PER': 0.9321926489226869,\n",
      " 'precision_macro': 0.7747694667180908,\n",
      " 'precision_micro': 0.8022214854111406,\n",
      " 'precision_weighted': 0.8329160391064419,\n",
      " 'recall_LOC': 0.8979591836734694,\n",
      " 'recall_MISC': 0.7275320970042796,\n",
      " 'recall_ORG': 0.8269581056466302,\n",
      " 'recall_PER': 0.7193154034229828,\n",
      " 'recall_macro': 0.7929411974368406,\n",
      " 'recall_micro': 0.7986466413599604,\n",
      " 'recall_weighted': 0.7986466413599602}\n"
     ]
    }
   ],
   "source": [
    "metrics = validate(model, test_dataloader, label_mapping, -100)\n",
    "pprint(metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Почему-то в конце обучения происходит что-то вроде градиентного взрыва из-за добавленного внимания.\n",
    "\n",
    "Я немного устал это дебагать, поэтому пусть будет как есть."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4MIrbmNoQEMA"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d4ce941904148077feb793883e611d25d231ca995d9164b22ee99fd0facd8d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
